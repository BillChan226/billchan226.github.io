<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* Design Modified from: Deepak Pathak, Jon Barron, and Saurabh Gupta. */
  /* UChicago-inspired color scheme */
  a {
  color: #800000; /* UChicago maroon for links */
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 400
  }
  heading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 17px; /* 19 */
    font-weight: 600 /* 1000 */
  }
  hr
  {
    border: 0;
    height: 2px;
    background-image: linear-gradient(to right, rgba(128, 0, 0, 0), rgba(128, 0, 0, 0.75), rgba(128, 0, 0, 0)); /* UChicago maroon for horizontal lines */
  }
  strong {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 600 /* 800 */
  }
  strongred {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    color: #800000; /* UChicago maroon */
    font-size: 16px
  }
  sectionheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    font-weight: 600;
    color: #000000; /* Changed from #800000 (maroon) to #000000 (black) */
  }
  pageheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 38px;
    font-weight: 400;
    color: #000000; /* Changed to black for your name */
  }
  .center {
    display: block;
    margin-left: auto;
    margin-right: auto;
    width: 80%;
  }
  .ImageBorder
  {
      border-width: 1px;
      border-color: #800000; /* UChicago maroon */
  }
  span.highlight {
  background-color: #ffffd0;
  }
  /* UChicago custom styles */
  .news-item b {
    color: #800000; /* Maroon for news dates */
  }
  table {
    border-collapse: separate;
    border-spacing: 0;
  }
  td {
    padding: 10px;
  }
  .paper-title {
    font-weight: 600;
    color: #800000; /* Maroon for paper titles */
  }
  .profile-box {
    background-color: #f8f8f8;
    border-radius: 10px;
    padding: 15px;
    border-left: 4px solid #800000; /* Maroon left border */
  }
  /* Add this refined news item styling */
  .news-container {
    margin-top: 15px;
  }

  .news-item {
    padding: 6px 0;
    border-bottom: 1px solid rgba(128, 0, 0, 0.1);
    margin-bottom: 6px;
    display: flex;
    align-items: baseline;
  }

  .news-date {
    color: #800000;
    font-weight: 600;
    min-width: 80px;
    margin-right: 8px;
    font-size: 0.95em;
  }

  .news-content {
    flex: 1;
    font-size: 0.98em;
    line-height: 1.4;
  }

  .news-content a {
    color: #800000;
    font-weight: 600;
    text-decoration: none;
  }

  .news-content a:hover {
    text-decoration: underline;
  }

  /* Refined publication button styles */
  .pub-buttons {
    margin-top: 5px;
    margin-bottom: 15px;
  }

  .pub-button {
    display: inline-block;
    padding: 4px 10px;
    margin-right: 5px;
    margin-bottom: 5px;
    border-radius: 4px;
    font-size: 0.9em;
    font-weight: 500;
    text-decoration: none;
    transition: all 0.2s ease;
  }

  .pub-button.primary {
    background-color: #800000;
    color: white;
  }

  .pub-button.primary:hover {
    background-color: #600000;
    color: white;
  }

  .pub-button.secondary {
    background-color: #555;
    color: white;
  }

  .pub-button.secondary:hover {
    background-color: #444;
    color: white;
  }

  .pub-button.text {
    background-color: transparent;
    color: #444;
  }

  .pub-button.text:hover {
    background-color: #f0f0f0;
    color: #800000;
  }

  /* Add this to your CSS section in the head */
  .presentation-badge {
    display: inline-block;
    font-size: 0.85em;
    font-weight: 600;
    color: white;
    background: linear-gradient(135deg, #800000, #a02020);
    padding: 3px 8px;
    border-radius: 4px;
    margin-top: 3px;
    box-shadow: 0 1px 3px rgba(0,0,0,0.1);
    letter-spacing: 0.02em;
  }
  </style>
  <link rel="shortcut icon" href="images/favicon.ico" type="image/vnd.microsoft.icon">
  <script type="text/javascript" src="js/hidebib.js"></script>
  <title>Zhaorun Chen, University of Chicago</title>
  <meta name="Zhaorun Chen's Homepage" http-equiv="Content-Type" content="Zhaorun Chen's Homepage">
  <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
</head>

<body>
<table width="1000" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr><td>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <p align="center">
    <pageheading>Zhaorun Chen</pageheading><br>
    <b>email</b>:&nbsp zhaorun (at) uchicago (dot) edu<br>
    <b>office</b>:&nbsp JCL 287, 5801 S Ellis Ave, Chicago, IL 60637
  </p>

  <tr>
    <td width="23%" valign="top" align="center"><a href="images/personal/profile.jpg"><img src="images/personal/profile.jpg" width="80%" style="border-radius:15px"></a>
    <p align=center>
    | <a href="data/CV_zhaorun.pdf">CV</a> |
    <a href="mailto:zhaorun@uchicago.edu">Email</a> |
    <a href="https://scholar.google.com/citations?user=UZg5N5UAAAAJ">Google Scholar</a> |
    <br/>
    <!-- | <a href="https://github.com/BillChan226">Github</a> | 
    <a href="https://www.linkedin.com/in/zhaorun-chen-1793b6226/">LinkedIn</a> | -->
    <!-- <a href="https://space.bilibili.com/14145636">Bilibili</a> |  -->
    </p>
    <p align="center" style="margin-top:-8px;"><iframe id="twitter-widget-0" scrolling="no" frameborder="0" allowtransparency="true" allowfullscreen="true" class="twitter-follow-button twitter-follow-button-rendered" style="position: static; visibility: visible; width: 186px; height: 20px;" title="Twitter Follow Button" src="https://platform.twitter.com/widgets/follow_button.2f70fb173b9000da126c79afe2098f02.en.html#dnt=false&amp;id=twitter-widget-0&amp;lang=en&amp;screen_name=ZRChen_AISafety&amp;show_count=false&amp;show_screen_name=true&amp;size=m&amp;time=1706734206165" data-screen-name=""></iframe><script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>
    </td>
    <td width="65%" valign="top" align="justify">
      <p>I am a first-year PhD student in the <a href="https://aisecure.github.io/">Secure Learning Lab</a> at the <a href="https://cs.uchicago.edu/">Department of Computer Science</a> at <a href="https://www.uchicago.edu/">University of Chicago</a> advised by Prof.<a href="https://aisecure.github.io/"> Bo Li</a>. I am also very fortunate to closely work with Prof. <a href="https://dawnsong.io/">Dawn Song</a> at UC Berkeley during my PhD.
      </p>
      <!-- <p>Previously, I received my Master degree in <a href="https://engineering.purdue.edu/ECE">Electrical and Computer Engineering</a> at<a href="https://www.purdue.edu/"> Purdue University</a>. Before that, I obtained my Bachelor degree in Automation at <a href="http://en.sjtu.edu.cn"> Shanghai Jiao Tong University</a>, advised by Prof.<a href="https://gaoyue.sjtu.edu.cn"> Yue Gao</a>. -->
        <p>Previously, I received my Bachelor degree at <a href="http://en.sjtu.edu.cn"> Shanghai Jiao Tong University</a> and Master degree at<a href="https://www.purdue.edu/"> Purdue University</a> both on computer engineering.
          <!-- I also work closely with Prof. <a href="https://www.huaxiuyao.io/">Huaxiu Yao</a> at UNC-Chapel Hill and Prof. <a href="https://dawnsong.io/">Dawn Song</a> at UC Berkeley. -->
        <!-- During 2023 Summer, I interned at <a href="https://cs.unc.edu"> UNC-Chapel Hill</a> advised by Prof.<a href="https://www.huaxiuyao.io/"> Huaxiu Yao</a> and collaborated some wonderful projects with <a href="https://irislab.stanford.edu/people.html"> IRIS Lab</a> hosted by Prof. <a href="https://ai.stanford.edu/~cbfinn/"> Chelsea Finn</a>. -->
      </p>
      <!-- <p>My current research interests center on the trustworthy and alignment issue of foundation models (e.g. LLMs) and agents from both a theoretical and application perspective. Specifically, I'm interested in enhancing their trustworthiness via novel algorithms and certificates for various applications (e.g. <b>hallucination mitigation</b>, <b>training & testing-time attack</b>, <b>guardrail models</b>) through incorporating external knowledge sources and LLMs' reasoning capabilities. -->
        <p>My research aims to build capable, safe, and trustworthy AI agents that learn from both observations and knowledge. I explore this through two pathways: (1) strengthening their internal intelligence and alignment by bootstrapping from experience and hindsight, and (2) applying external constraints grounded in human knowledge to achieve compliance and certifiability.
      </p>
      <!-- <p>I'm currently a research scientist intern at Meta Superintelligence Labs during this summer.
      </p> -->
      <!-- <p>Not right now, but one day:) -->
      <!-- </p> -->
      <!-- <p><a href="https://billchan226.github.io/publication.html">[Publications]</a> Email: zhaorun [AT] uchicago.edu -->
      </p>
    </td>
  </tr>
</table>

<!-- <hr/> -->
<hr/>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
  <tr>
    <td style="padding:2px;width:100%;vertical-align:middle">
      <heading>News</heading>
      <div class="news-container">
        <!-- <div class="news-item">
          <span class="news-date">[06/2025]</span>
          <!-- <span class="news-content">ðŸŽ‰ Two papers accepted by <a href="https://iccv.thecvf.com/">ICCV 2025</a>, see you in Hawaii!</span> -->
        <!-- </div>  -->

        <div class="news-item">
          <span class="news-date">[09/2025]</span>
          <span class="news-content">ðŸŽ‰ Seven papers accepted by <a href="https://neurips.cc/">NeurIPS 2025</a>. See you in San Diego!</span>
        </div>

        <div class="news-item">
          <span class="news-date">[06/2025]</span>
          <span class="news-content">ðŸ”¥ I'm currently a research scientist intern at <a href="https://ai.meta.com/">Meta Superintelligence Labs</a> during this summer!</span>
        </div>

        <div class="news-item">
          <span class="news-date">[05/2025]</span>
          <span class="news-content">ðŸŽ‰ One first-authored <a href="https://arxiv.org/abs/2503.22738">paper</a> accepted by <a href="https://icml.cc/">ICML 2025</a>!</span>
        </div>
        
        <div class="news-item">
          <span class="news-date">[01/2025]</span>
          <span class="news-content">ðŸŽ‰ Five papers accepted by <a href="https://iclr.cc/">ICLR 2025</a>, including two first-authored papers and one oral paper!</span>
        </div>
<!--         
        <div class="news-item">
          <span class="news-date">[09/2024]</span>
          <span class="news-content">ðŸŽ‰ Two papers accepted by <a href="https://neurips.cc/">NeurIPS 2024</a>!</span>
        </div>
        
        <div class="news-item">
          <span class="news-date">[05/2024]</span>
          <span class="news-content">ðŸŽ‰ One first-authored <a href="https://arxiv.org/pdf/2403.00425.pdf">paper</a> accepted by <a href="https://icml.cc/">ICML 2024</a>!</span>
        </div> --> 
        
        <!-- <div class="news-item" style="border-bottom: none;">
          <span class="news-date">[02/2024]</span>
          <span class="news-content">ðŸŽ‰ I've joined <a href="https://aisecure.github.io/">Secure Learning Lab</a> at UChicago as a Ph.D. student advised by Prof. <a href="https://aisecure.github.io/">Bo Li</a>.</span>
        </div> -->
      </div>
    </td>
  </tr>
</table>



<hr/>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Selected publications </sectionheading>(see full list on <a href="https://scholar.google.com/citations?user=UZg5N5UAAAAJ">Google Scholar</a>)</td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">
<br/>


<tr>
  <td width="30%" valign="top" align="center">
    <img src="images/mjbench/overview_new.png" 
         alt="sym" 
         width="90%"
         style="padding-top:0px; padding-bottom:0px; border-radius:15px;">
  </td>
  <td width="60%" valign="top">
    <p><a href="https://mj-bench.github.io/" id="mjbench">
    <heading>MJ-Bench: Is Your Multimodal Reward Model Really a Good Judge for Text-to-Image Generation?</heading></a><br>
    <b>Zhaorun Chen<sup>*</sup></b>, Yichao Du<sup>*</sup>, Zichen Wen<sup>*</sup>, Yiyang Zhou<sup>*</sup>, Chenhang Cui, Zhenzhen Weng, Haoqin Tu, Chaoqi Wang, Zhengwei Tong, Qinglan Huang, Canyu Chen, Qinghao Ye, Zhihong Zhu, Yuqing Zhang, Jiawei Zhou, Zhuokai Zhao, Rafael Rafailov, Chelsea Finn, and Huaxiu Yao<br>
    Advances in Neural Information Processing Systems (<b>NeurIPS</b>), 2025 <br>
    </p>

    <div class="paper" id="mjbench">
      <div class="pub-buttons">
        <a href="https://arxiv.org/pdf/2407.04842" class="pub-button primary">pdf</a>
        <a href="https://mj-bench.github.io/" class="pub-button secondary">webpage</a>
        <a href="javascript:toggleblock('mjbench_abs')" class="pub-button text">abstract</a>
        <a href="https://arxiv.org/abs/2407.04842" class="pub-button text">arXiv</a>
        <a href="https://www.alphaxiv.org/abs/2407.04842" class="pub-button text">alphaxiv</a>
        <a href="https://x.com/HuaxiuYaoML/status/1810728309182861462" class="pub-button text">blog</a> 
      </div>

      <p align="justify"> <i id="mjbench_abs">Multimodal reward models (RMs) are critical in RLHF and RLAIF, where they serve as judges and provide feedback for aligning foundation models (FMs) with desired behaviors. Despite their significance, these multimodal judges often un- dergo inadequate evaluation of their capabilities and biases, which may lead to potential misalignment and unsafe fine-tuning outcomes. To address this issue, we introduce MJ-Bench, a novel benchmark which incorporates a comprehensive preference dataset to evaluate multimodal judges in providing feedback for image generation models across four key perspectives: alignment, safety, image quality, and bias. Specifically, we evaluate a large variety of multimodal judges includ- ing smaller-sized CLIP-based scoring models, open-source VLMs (e.g. LLaVA family), and close-source VLMs (e.g. GPT-4o, Claude 3) on each decomposed subcategory of our preference dataset. Experiments reveal that close-source VLMs generally provide better feedback, with GPT-4o outperforming other judges in aver- age. Compared with open-source VLMs, smaller-sized scoring models can provide better feedback regarding text-image alignment and image quality, while VLMs provide more accurate feedback regarding safety and generation bias due to their stronger reasoning capabilities. Notably, human evaluations on end-to-end fine- tuned models using separate feedback from these multimodal judges provide similar conclusions, further confirming the effectiveness of MJ-Bench. Further studies in feedback scale reveal that VLM judges can generally provide more accurate and stable feedback in natural language (Likert-scale) than numerical scales. The code and data are available <a href="https://github.com/BillChan226/MJ-Bench" style="color:#36AE7C;">here</a>.</i></p>

<pre xml:space="preserve">
@misc{chen2024mjbenchmultimodalrewardmodel,
  title={MJ-Bench: Is Your Multimodal Reward Model Really a Good Judge for Text-to-Image Generation?}, 
  author={Zhaorun Chen and Yichao Du and Zichen Wen and Yiyang Zhou and Chenhang Cui and Zhenzhen Weng and Haoqin Tu and Chaoqi Wang and Zhengwei Tong and Qinglan Huang and Canyu Chen and Qinghao Ye and Zhihong Zhu and Yuqing Zhang and Jiawei Zhou and Zhuokai Zhao and Rafael Rafailov and Chelsea Finn and Huaxiu Yao},
  year={2024},
  eprint={2407.04842},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2407.04842}, 
}
</pre>
    </div>
  </td>
</tr>

<tr>
  <td width="30%" valign="top" align="center">
    <img src="images/guardsetx/overview.png" 
         alt="sym" 
         width="90%"
         style="padding-top:0px; padding-bottom:0px; border-radius:15px;">
  </td>
  <td width="60%" valign="top">
    <p><a href="" id="guardsetx">
    <heading>GuardSet-X: Massive Multi-Domain Safety Policy-Grounded Guardrail Dataset</heading></a><br>
    Mintong Kang<sup>*</sup>, <b>Zhaorun Chen<sup>*</sup></b>, Chejian Xu<sup>*</sup>, Jiawei Zhang<sup>*</sup>, Chengquan Guo<sup>*</sup>, Minzhou Pan, Ivan Revilla, Yu Sun, Bo Li<br>
    Advances in Neural Information Processing Systems (<b>NeurIPS</b>), 2025 <br>
    </p>

    <div class="paper" id="guardsetx">
      <div class="pub-buttons">
        <a href="https://arxiv.org/pdf/2506.19054" class="pub-button primary">pdf</a>
        <a href="" class="pub-button secondary">webpage</a>
        <a href="javascript:toggleblock('guardsetxabs')" class="pub-button text">abstract</a>
        <a href="https://arxiv.org/abs/2506.19054" class="pub-button text">arXiv</a>
        <!-- <a href="https://www.alphaxiv.org/abs/2506.19054" class="pub-button text">alphaxiv</a> -->
        <a href="" class="pub-button text">blog</a> 
      </div>

      <p align="justify"> <i id="guardsetxabs">As LLMs become widespread across diverse applications, concerns about the security and safety of LLM interactions have intensified. Numerous guardrail models and benchmarks have been developed to ensure LLM content safety. However, existing guardrail benchmarks are often built upon ad hoc risk taxonomies that lack a principled grounding in standardized safety policies, limiting their alignment with real-world operational requirements. Moreover, they tend to overlook domain-specific risks, while the same risk category can carry different implications across different domains. To bridge these gaps, we introduce GuardSet-X, the first massive multi-domain safety policy-grounded guardrail dataset. GuardSet-X offers: (1) broad domain coverage across eight safety-critical domains, such as finance, law, and codeGen; (2) policy-grounded risk construction based on authentic, domain-specific safety guidelines; (3) diverse interaction formats, encompassing declarative statements, questions, instructions, and multi-turn conversations; (4) advanced benign data curation via detoxification prompting to challenge over-refusal behaviors; and (5) \textbf{attack-enhanced instances} that simulate adversarial inputs designed to bypass guardrails. Based on GuardSet-X, we benchmark 19 advanced guardrail models and uncover a series of findings, such as: (1) All models achieve varied F1 scores, with many demonstrating high variance across risk categories, highlighting their limited domain coverage and insufficient handling of domain-specific safety concerns; (2) As models evolve, their coverage of safety risks broadens, but performance on common risk categories may decrease; (3) All models remain vulnerable to optimized adversarial attacks. We believe that \dataset and the unique insights derived from our evaluations will advance the development of policy-aligned and resilient guardrail systems.<i></p>

<pre xml:space="preserve">
  @article{kang2025polyguard,
    title={GuardSet-X: Massive Multi-Domain Safety Policy-Grounded Guardrail Dataset},
    author={Kang, Mintong and Chen, Zhaorun and Xu, Chejian and Zhang, Jiawei and Guo, Chengquan and Pan, Minzhou and Revilla, Ivan and Sun, Yu and Li, Bo},
    journal={arXiv preprint arXiv:2506.19054},
    year={2025}
}
</pre>
    </div>
  </td>
</tr>

<tr>
  <td width="30%" valign="top" align="center">
    <img src="images/mjvideo/overview.png" 
         alt="sym" 
         width="90%"
         style="padding-top:0px; padding-bottom:0px; border-radius:15px;">
  </td>
  <td width="60%" valign="top">
    <p><a href="https://aiming-lab.github.io/MJ-VIDEO.github.io/" id="mjvideo">
    <heading>MJ-Video: Benchmarking and Rewarding Video Generation with Fine-Grained Video Preference</heading></a><br>
    Haibo Tong<sup>*</sup>, Zhaoyang Wang<sup>*</sup>, <b>Zhaorun Chen</b>, Haonian Ji, Shi Qiu, Siwei Han, Kexin Geng, Zhongkai Xue, Yiyang Zhou, Peng Xia, Mingyu Ding, Rafael Rafailov, Chelsea Finn, and Huaxiu Yao<br>
    Advances in Neural Information Processing Systems (<b>NeurIPS</b>), 2025 <br><span style="color:red;"><b>(Spotlight Presentation)</b></span>
    </p>

    <div class="paper" id="mjvideo">
      <div class="pub-buttons">
        <a href="https://arxiv.org/pdf/2502.01719" class="pub-button primary">pdf</a>
        <a href="https://aiming-lab.github.io/MJ-VIDEO.github.io/" class="pub-button secondary">webpage</a>
        <a href="javascript:toggleblock('mjvideo_abs')" class="pub-button text">abstract</a>
        <a href="https://arxiv.org/abs/2502.01719" class="pub-button text">arXiv</a>
        <a href="https://www.alphaxiv.org/abs/2502.01719" class="pub-button text">alphaxiv</a>
        <a href="" class="pub-button text">blog</a> 
      </div>

      <p align="justify"> <i id="mjvideo_abs">Recent advancements in video generation have significantly improved the ability to synthesize videos from text instructions. However, existing models still struggle with key challenges such as instruction misalignment, content hallucination, safety concerns, and bias. Addressing these limitations, we introduce MJ-BENCH-VIDEO, a large-scale video preference benchmark designed to evaluate video generation across five critical aspects: Alignment, Safety, Fineness, Coherence & Consistency, and Bias & Fairness. This benchmark incorporates 28 fine-grained criteria to provide a comprehensive evaluation of video preference. Building upon this dataset, we propose MJ-VIDEO, a Mixture-of-Experts (MoE)-based video reward model designed to deliver fine-grained reward. MJ-VIDEO can dynamically select relevant experts to accurately judge the preference based on the input text-video pair. This architecture enables more precise and adaptable preference judgments. Through extensive benchmarking on MJ-BENCH-VIDEO, we analyze the limitations of existing video reward models and demonstrate the superior performance of MJ-VIDEO in video preference assessment, achieving 17.58% and 15.87% improvements in overall and fine-grained preference judgments, respectively. Additionally, introducing MJ-VIDEO for preference tuning in video generation enhances the alignment performance.<i></p>

<pre xml:space="preserve">
  @misc{tong2025mjvideofinegrainedbenchmarkingrewarding,
    title={MJ-VIDEO: Fine-Grained Benchmarking and Rewarding Video Preferences in Video Generation}, 
    author={Haibo Tong and Zhaoyang Wang and Zhaorun Chen and Haonian Ji and Shi Qiu and Siwei Han and Kexin Geng and Zhongkai Xue and Yiyang Zhou and Peng Xia and Mingyu Ding and Rafael Rafailov and Chelsea Finn and Huaxiu Yao},
    year={2025},
    eprint={2502.01719},
    archivePrefix={arXiv},
    primaryClass={cs.CV},
    url={https://arxiv.org/abs/2502.01719}, 
}
</pre>
    </div>
  </td>
</tr>

<tr>
  <td width="30%" valign="top" align="center">
    <img src="images/autoredteamer/method.png" 
         alt="sym" 
         width="90%"
         style="padding-top:0px; padding-bottom:0px; border-radius:15px;">
  </td>
  <td width="60%" valign="top">
    <p><a href="https://autoredteamer.com/" id="autoredteamer">
    <heading>AutoRedTeamer: Autonomous Red Teaming with Lifelong Attack Integration</heading></a><br>
    Andy Zhou, Kevin Wu, Francesco Pinto, <b>Zhaorun Chen</b>, Yi Zeng, Yu Yang, Shuang Yang, Sanmi Koyejo, James Zou, Bo Li<br>
    Advances in Neural Information Processing Systems (<b>NeurIPS</b>), 2025 <br>
    </p>

    <div class="paper" id="autoredteamer">
      <div class="pub-buttons">
        <a href="https://arxiv.org/pdf/2503.15754" class="pub-button primary">pdf</a>
        <a href="" class="pub-button secondary">webpage</a>
        <a href="javascript:toggleblock('autoredteamerabs')" class="pub-button text">abstract</a>
        <a href="https://arxiv.org/abs/2503.15754" class="pub-button text">arXiv</a>
        <!-- <a href="https://www.alphaxiv.org/abs/2506.19054" class="pub-button text">alphaxiv</a> -->
        <a href="" class="pub-button text">blog</a> 
      </div>

      <p align="justify"> <i id="autoredteamerabs">As large language models (LLMs) become increasingly capable, security and safety evaluation are crucial. While current red teaming approaches have made strides in assessing LLM vulnerabilities, they often rely heavily on human input and lack comprehensive coverage of emerging attack vectors. This paper introduces AutoRedTeamer, a novel framework for fully automated, end-to-end red teaming against LLMs. AutoRedTeamer combines a multi-agent architecture with a memory-guided attack selection mechanism to enable continuous discovery and integration of new attack vectors. The dual-agent framework consists of a red teaming agent that can operate from high-level risk categories alone to generate and execute test cases and a strategy proposer agent that autonomously discovers and implements new attacks by analyzing recent research. This modular design allows AutoRedTeamer to adapt to emerging threats while maintaining strong performance on existing attack vectors. We demonstrate AutoRedTeamer's effectiveness across diverse evaluation settings, achieving 20% higher attack success rates on HarmBench against Llama-3.1-70B while reducing computational costs by 46% compared to existing approaches. AutoRedTeamer also matches the diversity of human-curated benchmarks in generating test cases, providing a comprehensive, scalable, and continuously evolving framework for evaluating the security of AI systems.<i></p>

<pre xml:space="preserve">
  @article{zhou2025autoredteamer,
    title={Autoredteamer: Autonomous red teaming with lifelong attack integration},
    author={Zhou, Andy and Wu, Kevin and Pinto, Francesco and Chen, Zhaorun and Zeng, Yi and Yang, Yu and Yang, Shuang and Koyejo, Sanmi and Zou, James and Li, Bo},
    journal={arXiv preprint arXiv:2503.15754},
    year={2025}
  }
</pre>
    </div>
  </td>
</tr>

<tr>
  <td width="30%" valign="top" align="center">
    <img src="images/dropout/method.png" 
         alt="sym" 
         width="90%"
         style="padding-top:0px; padding-bottom:0px; border-radius:15px;">
  </td>
  <td width="60%" valign="top">
    <p><a href="" id="dropoutdecoding">
    <heading>From Uncertainty to Trust: Enhancing Reliability in Vision-Language Models with Uncertainty-Guided Dropout Decoding</heading></a><br>
    Yixiong Fang, Ziran Yang, <b>Zhaorun Chen</b>, Zhuokai Zhao, Jiawei Zhou<br>
    Advances in Neural Information Processing Systems (<b>NeurIPS</b>), 2025 <br>
    </p>

    <div class="paper" id="dropoutdecoding">
      <div class="pub-buttons">
        <a href="https://arxiv.org/pdf/2412.06474" class="pub-button primary">pdf</a>
        <a href="" class="pub-button secondary">webpage</a>
        <a href="javascript:toggleblock('dropoutdecodingabs')" class="pub-button text">abstract</a>
        <a href="https://arxiv.org/abs/2412.06474" class="pub-button text">arXiv</a>
        <!-- <a href="https://www.alphaxiv.org/abs/2506.19054" class="pub-button text">alphaxiv</a> -->
        <a href="" class="pub-button text">blog</a> 
      </div>

      <p align="justify"> <i id="dropoutdecodingabs">Large vision-language models (LVLMs) demonstrate remarkable capabilities in multimodal tasks but are prone to misinterpreting visual inputs, often resulting in hallucinations and unreliable outputs. To address these challenges, we propose Dropout Decoding, a novel inference-time approach that quantifies the uncertainty of visual tokens and selectively masks uncertain tokens to improve decoding. Our method measures the uncertainty of each visual token by projecting it onto the text space and decomposing it into aleatoric and epistemic components. Specifically, we focus on epistemic uncertainty, which captures perception-related errors more effectively. Inspired by dropout regularization, we introduce uncertainty-guided token dropout, which applies the dropout principle to input visual tokens instead of model parameters, and during inference rather than training. By aggregating predictions from an ensemble of masked decoding contexts, Dropout Decoding robustly mitigates errors arising from visual token misinterpretations. Evaluations on benchmarks including CHAIR, THRONE, and MMBench demonstrate that Dropout Decoding significantly reduces object hallucinations (OH) and enhances both reliability and quality of LVLM outputs across diverse visual contexts.<i></p>

<pre xml:space="preserve">
  @article{fang2024uncertainty,
    title={From uncertainty to trust: Enhancing reliability in vision-language models with uncertainty-guided dropout decoding},
    author={Fang, Yixiong and Yang, Ziran and Chen, Zhaorun and Zhao, Zhuokai and Zhou, Jiawei},
    journal={arXiv preprint arXiv:2412.06474},
    year={2024}
  }
</pre>
    </div>
  </td>
</tr>

<tr>
  <td width="30%" valign="top" align="center">
    <img src="images/shieldagent/overview.png" 
         alt="sym" 
         width="90%"
         style="padding-top:0px; padding-bottom:0px; border-radius:15px;">
  </td>
  <td width="60%" valign="top">
    <p><a href="https://shieldagent-aiguard.github.io/" id="shieldagent">
    <heading>ShieldAgent: Shielding Agents via Verifiable Safety Policy Reasoning</heading></a><br>
    <b>Zhaorun Chen</b>, Mintong Kang, and Bo Li<br>
    International Conference on Machine Learning (<b>ICML</b>), 2025<br>
    </p>

    <div class="paper" id="shieldagent">
      <div class="pub-buttons">
        <a href="https://arxiv.org/abs/2503.22738" class="pub-button primary">pdf</a>
        <a href="https://shieldagent-aiguard.github.io/" class="pub-button secondary">webpage</a>
        <a href="javascript:toggleblock('shieldagent_abs')" class="pub-button text">abstract</a>
        <a href="https://arxiv.org/abs/2503.22738" class="pub-button text">arXiv</a>
        <a href="https://www.alphaxiv.org/abs/2503.22738" class="pub-button text">alphaxiv</a>
        <a href="" class="pub-button text">blog</a> 
      </div>

      <p align="justify"> <i id="shieldagent_abs">Autonomous agents powered by foundation models have seen widespread adoption across various real-world applications. However, they remain highly vulnerable to malicious instructions and attacks, which can result in severe consequences such as privacy breaches and financial losses. More critically, existing guardrails for LLMs are not applicable due to the complex and dynamic nature of agents. To tackle these challenges, we propose ShieldAgent, the first guardrail agent designed to enforce explicit safety policy compliance for the action trajectory of other protected agents through logical reasoning. Specifically, ShieldAgent first constructs a safety policy model by extracting verifiable rules from policy documents and structuring them into a set of action-based probabilistic rule circuits. Given the action trajectory of the protected agent, ShieldAgent retrieves relevant rule circuits and generates a shielding plan, leveraging its comprehensive tool library and executable code for formal verification. In addition, given the lack of guardrail benchmarks for agents, we introduce ShieldAgent-Bench, a dataset with 3K safety-related pairs of agent instructions and action trajectories, collected via SOTA attacks across 6 web environments and 7 risk categories. Experiments show that ShieldAgent achieves SOTA on ShieldAgent-Bench and three existing benchmarks, outperforming prior methods by 11.3% on average with a high recall of 90.1%. Additionally, ShieldAgent reduces API queries by 64.7% and inference time by 58.2%, demonstrating its high precision and efficiency in safeguarding agents.<i></p>

<pre xml:space="preserve">
  @article{chen2025shieldagent,
    title={ShieldAgent: Shielding Agents via Verifiable Safety Policy Reasoning},
    author={Chen, Zhaorun and Kang, Mintong and Li, Bo},
    journal={arXiv preprint arXiv:2503.22738},
    year={2025}
  }
</pre>
    </div>
  </td>
</tr>

<tr>
  <td width="30%" valign="top" align="center">
    <video playsinline 
           autoplay 
           loop 
           muted 
           src="images/safewatch/website_demonstration.mov"
           width="90%"
           style="padding-top:0px; padding-bottom:0px; border-radius:15px; border: 1px solid #800000;">
    </video>
  </td>
  <td width="60%" valign="top">
    <p><a href="https://safewatch-aiguard.github.io/" id="SafeWatch" class="paper-title">
    <heading>SafeWatch: An Efficient Safety-Policy Following Video Guardrail Model with Transparent Explanations</heading></a><br>
    <b>Zhaorun Chen</b>, Francesco Pinto, Minzhou Pan, and Bo Li<br>
    International Conference on Learning Representations (<b>ICLR</b>), 2025<br>
    </p>

    <div class="paper" id="safewatch">
      <div class="pub-buttons">
        <a href="https://arxiv.org/pdf/2412.06878" class="pub-button primary">pdf</a>
        <a href="https://safewatch-aiguard.github.io/" class="pub-button secondary">webpage</a>
        <a href="javascript:toggleblock('safewatch_abs')" class="pub-button text">abstract</a>
        <a href="https://arxiv.org/abs/2412.06878" class="pub-button text">arXiv</a>
        <a href="https://www.alphaxiv.org/abs/2412.06878" class="pub-button text">alphaxiv</a>
        <a href="https://x.com/ZRChen_AISafety/status/1869621962223616130" class="pub-button text">blog</a>
      </div>

      <p align="justify"> <i id="safewatch_abs">With the rise of generative AI and rapid growth of high-quality video generation, video guardrails have become more crucial than ever to ensure safety and security across platforms. Current video guardrails, however, are either overly simplistic, relying on pure classification models trained on simple policies with limited unsafe categories, which lack detailed explanations, or prompting multimodal large language models (MLLMs) with long safety guidelines, which are inefficient and impractical for guardrailing real-world content. To bridge this gap, we propose SafeWatch, an efficient MLLM-based video guardrail model designed to follow customized safety policies and provide multi-label video guardrail outputs with content-specific explanations in a zero-shot manner. In particular, unlike traditional MLLM-based guardrails that encode all safety policies autoregressively, causing inefficiency and bias, SafeWatch uniquely encodes each policy chunk in parallel and eliminates their position bias such that all policies are attended simultaneously with equal importance. In addition, to improve efficiency and accuracy, SafeWatch incorporates a policy-aware visual token pruning algorithm that adaptively selects the most relevant video tokens for each policy, discarding noisy or irrelevant information. This allows for more focused, policy-compliant guardrail with significantly reduced computational overhead. Considering the limitations of existing video guardrail benchmarks, we propose SafeWatch-Bench, a large-scale video guardrail benchmark comprising over 2M videos spanning six safety categories which covers over 30 tasks to ensure a comprehensive coverage of all potential safety scenarios. We have conducted extensive experiments, showing that SafeWatch outperforms all SOTA video guardrails on SafeWatch-Bench by 28.2%, and achieves a 13.6% improvement on existing benchmarks, all while reducing inference costs by an average of 10%. SafeWatch also demonstrates strong policy-following abilities and outperforms previous SOTAs by 5.6% and 15.6% in zero-shot generalizability to new policies and new prompting tasks. Additionally, both LLM-as-a-judge and human evaluators confirm the high quality of the explanations provided by SafeWatch. Our project is open-sourced at <a href="https://safewatch-aiguard.github.io" style="color:#800000;">here</a>.</i></p>

<pre xml:space="preserve">
@article{chen2024safewatch,
  title={SafeWatch: An Efficient Safety-Policy Following Video Guardrail Model with Transparent Explanations},
  author={Chen, Zhaorun and Pinto, Francesco and Pan, Minzhou and Li, Bo},
  journal={arXiv preprint arXiv:2412.06878},
  year={2024}
}
</pre>
    </div>
  </td>
</tr>


<tr>
  <td width="30%" valign="top" align="center">
    <img src="images/mmdt/mmdt.png" 
         alt="sym" 
         width="90%"
         style="padding-top:0px; padding-bottom:0px; border-radius:15px;">
  </td>
  <td width="60%" valign="top">
    <p><a href="https://mmdecodingtrust.github.io/" id="MMDT">
    <heading>MMDT: Decoding the Trustworthiness and Safety of Multimodal Foundation Models</heading></a><br>
    Chejian Xu<sup>*</sup>, Jiawei Zhang<sup>*</sup>, <b>Zhaorun Chen<sup>*</sup></b>, Chulin Xie<sup>*</sup>, Mintong Kang<sup>*</sup>, Yujin Potter<sup>*</sup>, Zhun Wang<sup>*</sup>, Zhuowen Yuan<sup>*</sup>, Alexander Xiong, Zidi Xiong, Chenhui Zhang, Lingzhi Yuan, Yi Zeng, Peiyang Xu, Chengquan Guo, Andy Zhou, Jeffrey Ziwei Tan, Xuandong Zhao, Francesco Pinto, Zhen Xiang, Yu Gai, Zinan Lin, Dan Hendrycks, Bo Li, Dawn Song<br>
    International Conference on Learning Representations (<b>ICLR</b>), 2025<br>
    </p>

    <div class="paper" id="mmdt">
      <div class="pub-buttons">
        <a href="https://openreview.net/pdf?id=qIbbBSzH6n" class="pub-button primary">pdf</a>
        <a href="https://mmdecodingtrust.github.io/" class="pub-button secondary">webpage</a>
        <a href="javascript:toggleblock('mmdt_abs')" class="pub-button text">abstract</a>
        <a href="" class="pub-button text">arXiv</a>
      </div>

      <p align="justify"> <i id="mmdt_abs">Multimodal foundation models (MMFMs) play a crucial role in various applications, including autonomous driving, healthcare, and virtual assistants. However, several studies have revealed vulnerabilities in these models, such as generating unsafe content by text-to-image models. Existing benchmarks on multimodal models either predominantly assess the helpfulness of these models, or only focus on limited perspectives such as fairness and privacy. In this paper, we present the first unified platform, MMDT (Multimodal DecodingTrust), designed to provide a comprehensive safety and trustworthiness evaluation for MMFMs. Our platform assesses models from multiple perspectives, including safety, hallucination, fairness/bias, privacy, adversarial robustness, and out-of-distribution (OOD) generalization. We have designed various evaluation scenarios and red teaming algorithms under different tasks for each perspective to generate challenging data, forming a high-quality benchmark. We evaluate a range of multimodal models using MMDT, and our findings reveal a series of vulnerabilities and areas for improvement across these perspectives. This work introduces the first comprehensive and unique safety and trustworthiness evaluation platform for MMFMs, paving the way for developing safer and more reliable MMFMs and systems.</i></p>

<pre xml:space="preserve">
@inproceedings{
  xu2025mmdt,
  title={{MMDT}: Decoding the Trustworthiness and Safety of Multimodal Foundation Models},
  author={Chejian Xu and Jiawei Zhang and Zhaorun Chen and Chulin Xie and Mintong Kang and Yujin Potter and Zhun Wang and Zhuowen Yuan and Alexander Xiong and Zidi Xiong and Chenhui Zhang and Lingzhi Yuan and Yi Zeng and Peiyang Xu and Chengquan Guo and Andy Zhou and Jeffrey Ziwei Tan and Xuandong Zhao and Francesco Pinto and Zhen Xiang and Yu Gai and Zinan Lin and Dan Hendrycks and Bo Li and Dawn Song},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2025},
  url={https://openreview.net/forum?id=qIbbBSzH6n}
  }
</pre>
    </div>
  </td>
</tr>

<tr></tr>
  <td width="30%" valign="top" align="center">
    <img src="images/mmie/mmie.jpg" 
         alt="sym" 
         width="90%"
         style="padding-top:0px; padding-bottom:0px; border-radius:15px;">
  </td>
  <td width="60%" valign="top">
    <p><a href="https://mmie-bench.github.io/" id="MMIE">
    <heading>MMIE: Massive Multimodal Interleaved Comprehension Benchmark For Large Vision-Language Models</heading></a><br>
    Peng Xia<sup>*</sup>, Siwei Han<sup>*</sup>, Shi Qiu<sup>*</sup>, Yiyang Zhou, Zhaoyang Wang, Wenhao Zheng, <b>Zhaorun Chen</b>, Chenhang Cui, Mingyu Ding, Linjie Li, Lijuan Wang, Huaxiu Yao<br>
    <!-- International Conference on Learning Representations (<b>ICLR</b>), 2025 <br>
    <span class="presentation-badge">Oral Presentation</span> -->
    International Conference on Learning Representations (<b>ICLR</b>), 2025 <br><span style="color:red;"><b>(Oral Presentation)</b></span>
    </p>

    <div class="paper" id="mmie" style="margin-top: -17px;">
      <div class="pub-buttons">
        <a href="https://arxiv.org/pdf/2410.10139" class="pub-button primary">pdf</a>
        <a href="https://mmie-bench.github.io/" class="pub-button secondary">webpage</a>
        <a href="javascript:toggleblock('mmie_abs')" class="pub-button text">abstract</a>
        <a href="https://arxiv.org/pdf/2410.10139" class="pub-button text">arXiv</a>
        <a href="https://www.alphaxiv.org/abs/2410.10139" class="pub-button text">alphaxiv</a>
        <a href="https://x.com/HuaxiuYaoML/status/1846014867129434259" class="pub-button text">blog</a>
      </div>

      <p align="justify"> <i id="mmie_abs">Interleaved multimodal comprehension and generation, enabling models to produce and interpret both images and text in arbitrary sequences, have become a pivotal area in multimodal learning. Despite significant advancements, the evaluation of this capability remains insufficient. Existing benchmarks suffer from limitations in data scale, scope, and evaluation depth, while current evaluation metrics are often costly or biased, lacking in reliability for practical applications. To address these challenges, we introduce MMIE, a large-scale knowledge-intensive benchmark for evaluating interleaved multimodal comprehension and generation in Large Vision-Language Models (LVLMs). MMIE comprises 20K meticulously curated multimodal queries, spanning 3 categories, 12 fields, and 102 subfields, including mathematics, coding, physics, literature, health, and arts. It supports both interleaved inputs and outputs, offering a mix of multiple-choice and open-ended question formats to evaluate diverse competencies. Moreover, we propose a reliable automated evaluation metric, leveraging a scoring model fine-tuned with human-annotated data and systematic evaluation criteria, aimed at reducing bias and improving evaluation accuracy. Extensive experiments demonstrate the effectiveness of our benchmark and metrics in providing a comprehensive evaluation of interleaved LVLMs. Specifically, we evaluate eight LVLMs, revealing that even the best models show significant room for improvement, with most achieving only moderate results. We believe MMIE will drive further advancements in the development of interleaved LVLMs.</i></p>

<pre xml:space="preserve">
@article{xia2024mmie,
  title={MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models},
  author={Xia, Peng and Han, Siwei and Qiu, Shi and Zhou, Yiyang and Wang, Zhaoyang and Zheng, Wenhao and Chen, Zhaorun and Cui, Chenhang and Ding, Mingyu and Li, Linjie and Wang, Lijuan and Yao, Huaxiu},
  journal={arXiv preprint arXiv:2410.10139},
  year={2024}
}
</pre>
    </div>
  </td>
</tr>


<tr>
  <td width="30%" valign="top" align="center">
    <img src="images/agentpoison/method.png" 
         alt="sym" 
         width="90%"
         style="padding-top:0px; padding-bottom:0px; border-radius:15px;">
  </td>
  <td width="60%" valign="top">
    <p><a href="https://billchan226.github.io/AgentPoison.html" id="AgentPoison">
    <heading>AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases</heading></a><br>
    <b>Zhaorun Chen</b>, Zhen Xiang, Chaowei Xiao, Dawn Song, and Bo Li<br>
    Advances in Neural Information Processing Systems (<b>NeurIPS</b>), 2024<br>
    </p>

    <div class="paper" id="agentpoison">
      <div class="pub-buttons">
        <a href="https://arxiv.org/pdf/2407.12784" class="pub-button primary">pdf</a>
        <a href="https://billchan226.github.io/AgentPoison.html" class="pub-button secondary">webpage</a>
        <a href="javascript:toggleblock('agentpoison_abs')" class="pub-button text">abstract</a>
        <a href="https://arxiv.org/abs/2310.03379" class="pub-button text">arXiv</a>
        <a href="https://www.alphaxiv.org/abs/2407.12784" class="pub-button text">alphaxiv</a>
        <a href="https://x.com/ZRChen_AISafety/status/1814412843904840095" class="pub-button text">blog</a>
      </div>

      <p align="justify"> <i id="agentpoison_abs">LLM agents have demonstrated remarkable performance across various applications, primarily due to their advanced capabilities in reasoning, utilizing external knowledge and tools, calling APIs, and executing actions to interact with environments. Current agents typically utilize a memory module or a retrieval-augmented generation (RAG) mechanism, retrieving past knowledge and instances with similar embeddings from knowledge bases to inform task planning and execution. However, the reliance on unverified knowledge bases raises significant concerns about their safety and trustworthiness. To uncover such vulnerabilities, we propose a novel red teaming approach AgentPoison, the first backdoor attack targeting generic and RAG-based LLM agents by poisoning their long-term memory or RAG knowledge base. In particular, we form the trigger generation process as a constrained optimization to optimize backdoor triggers by mapping the triggered instances to a unique embedding space, so as to ensure that whenever a user instruction contains the optimized backdoor trigger, the malicious demonstrations are retrieved from the poisoned memory or knowledge base with high probability. In the meantime, benign instructions without the trigger will still maintain normal performance. Unlike conventional backdoor attacks, AgentPoison requires no additional model training or fine-tuning, and the optimized backdoor trigger exhibits superior transferability, in-context coherence, and stealthiness. Extensive experiments demonstrate AgentPoison effectiveness in attacking three types of real-world LLM agents: RAG-based autonomous driving agent, knowledge-intensive QA agent, and healthcare EHRAgent. We inject the poisoning instances into the RAG knowledge base and long-term memories of these agents, respectively, demonstrating the generalization of AgentPoison. On each agent, AgentPoison achieves an average attack success rate of â‰¥ 80% with minimal impact on benign performance (â‰¤ 1%) with a poison rate < 0.1%. Code is released <a href="https://github.com/BillChan226/AgentPoison" style="color:#36AE7C;">here</a>.</i></p>

<pre xml:space="preserve">
@article{chen2024agentpoison,
  title={AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases},
  author={Chen, Zhaorun and Xiang, Zhen and Xiao, Chaowei and Song, Dawn and Li, Bo},
  journal={arXiv preprint arXiv:2407.12784},
  year={2024}
}
</pre>
    </div>
  </td>
</tr>

<tr>
  <td width="30%" valign="top" align="center">
    <img src="images/halc/halc.png" 
         alt="sym" 
         width="90%"
         style="padding-top:0px; padding-bottom:0px; border-radius:15px;">
  </td>
  <td width="60%" valign="top">
    <p><a href="https://billchan226.github.io/HALC.html" id="HALC">
    <heading>HALC: Object Hallucination Reduction via Adaptive Focal-Contrast Decoding</heading></a><br>
    <b>Zhaorun Chen<sup>*</sup></b>, Zhuokai Zhao<sup>*</sup>, Hongyin Luo, Huaxiu Yao, Bo Li and Jiawei Zhou<br>
    International Conference on Machine Learning (<b>ICML</b>), 2024<br>
    <!-- short version presented at ICLR 2024 <a href="https://iclr-r2fm.github.io/">R2-FM</a> Workshop<br> -->
    </p>

    <div class="paper" id="halc">
      <div class="pub-buttons">
        <a href="https://arxiv.org/pdf/2403.00425" class="pub-button primary">pdf</a>
        <a href="https://billchan226.github.io/HALC.html" class="pub-button secondary">webpage</a>
        <a href="javascript:toggleblock('halc_abs')" class="pub-button text">abstract</a>
        <a href="https://arxiv.org/abs/2310.03379" class="pub-button text">arXiv</a>
        <a href="https://www.alphaxiv.org/abs/2403.00425" class="pub-button text">alphaxiv</a>
        <a href="https://www.virtueai.com/2024/07/29/halc-object-hallucination-reduction-via-adaptive-focal-contrast-decoding/" class="pub-button text">blog</a> 
      </div>

      <p align="justify"> <i id="halc_abs">While large vision-language models (LVLMs) have demonstrated impressive capabilities in interpreting multi-modal contexts, they invariably suffer from object hallucinations (OH). We introduce HALC, a novel decoding algorithm designed to mitigate OH in LVLMs. HALC leverages distinct fine-grained optimal visual information in vision-language tasks and operates on both local and global contexts simultaneously. Specifically, HALC integrates a robust auto-focal grounding mechanism (locally) to correct hallucinated tokens on the fly, and a specialized beam search algorithm (globally) to significantly reduce OH while preserving text generation quality. Additionally, HALC can be integrated into any LVLMs as a plug-and-play module without extra training. Extensive experimental studies demonstrate HALC's effectiveness in reducing OH, outperforming state-of-the-arts across four benchmarks. Code is released <a href="https://github.com/BillChan226/HALC" style="color:#36AE7C;">here</a>.</i></p>

<pre xml:space="preserve">
@article{chen2024halc,
  title={HALC: Object Hallucination Reduction via Adaptive Focal-Contrast Decoding},
  author={Chen, Zhaorun and Zhao, Zhuokai and Luo, Hongyin and Yao, Huaxiu and Li, Bo and Zhou, Jiawei},
  journal={arXiv preprint arXiv:2403.00425},
  year={2024}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="30%" valign="top" align="center">
    <img src="images/fisao/fisao.jpg" 
         alt="sym" 
         width="90%"
         style="padding-top:0px; padding-bottom:0px; border-radius:15px;">
  </td>
  <td width="60%" valign="top">
    <p><a href="" id="FineGrainedVerifiers">
    <heading>Fine-Grained Verifiers: Preference Modeling as Next-token Prediction in Vision-Language Alignment</heading></a><br>
    Chenhang Cui<sup>*</sup>, An Zhang<sup>*</sup>, Yiyang Zhou, <b>Zhaorun Chen</b>, Gelei Deng, Huaxiu Yao, Tat-Seng Chua<br>
    International Conference on Learning Representations (<b>ICLR</b>), 2025<br>
    </p>

    <div class="paper" id="anyprefer">
      <div class="pub-buttons">
        <a href="https://arxiv.org/pdf/2410.14148" class="pub-button primary">pdf</a>
        <a href="" class="pub-button secondary">webpage</a>
        <a href="javascript:toggleblock('fisao_abs')" class="pub-button text">abstract</a>
        <a href="https://arxiv.org/pdf/2410.14148" class="pub-button text">arXiv</a>
        <a href="https://www.alphaxiv.org/abs/2410.14148" class="pub-button text">alphaxiv</a>
        <!-- <a href="https://x.com/HuaxiuYaoML/status/1846014867129434259">blog</a> -->
      </div>

      <p align="justify"> <i id="fisao_abs">The recent advancements in large language models (LLMs) and pre-trained vision models have accelerated the development of vision-language large models (VLLMs), enhancing the interaction between visual and linguistic modalities. Despite their notable success across various domains, VLLMs face challenges in modality alignment, which can lead to issues like hallucinations and unsafe content generation. Current alignment techniques often rely on coarse feedback and external datasets, limiting scalability and performance. In this paper, we propose FiSAO (Fine-Grained Self-Alignment Optimization), a novel self-alignment method that utilizes the model's own visual encoder as a fine-grained verifier to improve vision-language alignment without the need for additional data. By leveraging token-level feedback from the vision encoder, FiSAO significantly improves vision-language alignment, even surpassing traditional preference tuning methods that require additional data. Through both theoretical analysis and experimental validation, we demonstrate that FiSAO effectively addresses the misalignment problem in VLLMs, marking the first instance of token-level rewards being applied to such models.</i></p>

<pre xml:space="preserve">
@article{cui2024fine,
  title={Fine-Grained Verifiers: Preference Modeling as Next-token Prediction in Vision-Language Alignment},
  author={Cui, Chenhang and Zhang, An and Zhou, Yiyang and Chen, Zhaorun and Deng, Gelei and Yao, Huaxiu and Chua, Tat-Seng},
  journal={arXiv preprint arXiv:2410.14148},
  year={2024}
}
</pre>
    </div>
  </td>
</tr>


<tr>
  <td width="30%" valign="top" align="center">
    <img src="images/anyprefer/anyprefer.jpg" 
         alt="sym" 
         width="90%"
         style="padding-top:0px; padding-bottom:0px; border-radius:15px;">
  </td>
  <td width="60%" valign="top">
    <p><a href="" id="AnyPrefer">
    <heading>Anyprefer: An Agentic Framework for Preference Data Synthesis</heading></a><br>
    Yiyang Zhou, Zhaoyang Wang, Tianle Wang, Shangyu Xing, Peng Xia, Bo Li, Kaiyuan Zheng, Zijian Zhang, <b>Zhaorun Chen</b>, Wenhao Zheng, Xuchao Zhang, Chetan Bansal, Weitong Zhang, Ying Wei, Mohit Bansal, Huaxiu Yao<br>
    International Conference on Learning Representations (<b>ICLR</b>), 2025<br>
    </p>

    <div class="paper" id="anyprefer">
      <div class="pub-buttons">
        <a href="" class="pub-button primary">pdf</a>
        <a href="" class="pub-button secondary">webpage</a>
        <a href="javascript:toggleblock('anyprefer_abs')" class="pub-button text">abstract</a>
        <a href="" class="pub-button text">arXiv</a>
        <!-- <a href="">alphaxiv</a> | -->
        <!-- <a href="https://x.com/HuaxiuYaoML/status/1846014867129434259">blog</a> -->
      </div>

      <p align="justify"> <i id="anyprefer_abs">High-quality preference data is essential for aligning foundation models with human values through preference learning. However, manual annotation of such data is often time-consuming and costly. Recent methods adopt a self-rewarding approach, where the target model generates and annotates its own preference data, but this can lead to inaccuracies due to the reward model sharing weights with the target model, amplifying inherent biases. To address these issues, we propose Anyprefer, a framework designed to synthesize high-quality preference data for the target model. Anyprefer frames the data synthesis process as a cooperative two-player Markov Game, where the target model and a judge model collaborate. Here, a series of external tools are introduced to assist the judge model in accurately rewarding the target model's responses, mitigating biases in the process. We also introduce a feedback mechanism to optimize prompts for both models, enhancing collaboration and improving data quality. The synthesized data is compiled into a new preference dataset, Anyprefer-V1, consisting of 58K high-quality preference pairs. Extensive experiments show that Anyprefer significantly improves model alignment across four applications, covering 21 datasets, achieving average improvements of 18.55 in five natural language generation datasets, 3.66 in nine vision-language understanding datasets, 30.05 in three medical image analysis datasets, and 14.50 in four visuo-motor control tasks.</i></p>

<pre xml:space="preserve">
@inproceedings{
  zhou2025mjprefergen,
  title={{MJ}-PreferGen: An Automatic Framework for Preference Data Synthesis},
  author={Yiyang Zhou and Zhaoyang Wang and Tianle Wang and Shangyu Xing and Peng Xia and Bo Li and Kaiyuan Zheng and Zijian Zhang and Zhaorun Chen and Wenhao Zheng and Xuchao Zhang and Chetan Bansal and Weitong Zhang and Ying Wei and Mohit Bansal and Huaxiu Yao},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2025},
  url={https://openreview.net/forum?id=WpZyPk79Fu}
  }
</pre>
    </div>
  </td>
</tr>




<tr>
  <td width="30%" valign="top" align="center">
    <img src="images/paper/csr.png" 
         alt="sym" 
         width="90%"
         style="padding-top:0px; padding-bottom:0px; border-radius:15px;">
  </td>
  <td width="60%" valign="top">
    <p><a href="https://dongjie-cheng.github.io/CSR.html" id="CSR">
    <heading>Calibrated Self-Rewarding Vision Language Models</heading></a><br>
    Yiyang Zhou<sup>*</sup>, Zhiyuan Fan<sup>*</sup>, Dongjie Cheng<sup>*</sup>, Sihan Yang, <b>Zhaorun Chen</b>, Chenhang Cui, Xiyao Wang, Yun Li, Linjun Zhang, and Huaxiu Yao<br>
    Advances in Neural Information Processing Systems (<b>NeurIPS</b>), 2024<br>
    </p>

    <div class="paper" id="csr">
      <div class="pub-buttons">
        <a href="https://arxiv.org/pdf/2405.14622" class="pub-button primary">pdf</a>
        <a href="https://dongjie-cheng.github.io/CSR.html" class="pub-button secondary">webpage</a>
        <a href="javascript:toggleblock('csr_abs')" class="pub-button text">abstract</a>
        <a href="https://arxiv.org/pdf/2405.14622" class="pub-button text">arXiv</a>
        <a href="https://www.alphaxiv.org/abs/2405.14622" class="pub-button text">alphaxiv</a>
        <a href="https://x.com/HuaxiuYaoML/status/1794203052116680895" class="pub-button text">blog</a>
      </div>

      <p align="justify"> <i id="csr_abs">Large Vision-Language Models (LVLMs) have made substantial progress by integrating pre-trained large language models (LLMs) and vision models through instruction tuning. Despite these advancements, LVLMs often exhibit the hallucination phenomenon, where generated text responses appear linguistically plausible but contradict the input image, indicating a misalignment between image and text pairs. This misalignment arises because the model tends to prioritize textual information over visual input, even when both the language model and visual representations are of high quality. Existing methods leverage additional models or human annotations to curate preference data and enhance modality alignment through preference optimization. These approaches are resource-intensive and may not effectively reflect the target LVLM's preferences, making the curated preferences easily distinguishable. Our work addresses these challenges by proposing the Calibrated Self-Rewarding (CSR) approach, which enables the model to self-improve by iteratively generating candidate responses, evaluating the reward for each response, and curating preference data for fine-tuning. In the reward modeling, we employ a step-wise strategy and incorporate visual constraints into the self-rewarding process to place greater emphasis on visual input. Empirical results demonstrate that CSR significantly enhances performance and reduces hallucinations across ten benchmarks and tasks, achieving substantial improvements over existing methods by 7.62%. Our empirical results are further supported by rigorous theoretical analysis, under mild assumptions, verifying the effectiveness of introducing visual constraints into the self-rewarding paradigm. Additionally, CSR shows compatibility with different vision-language models and the ability to incrementally improve performance through iterative fine-tuning.</i></p>

<pre xml:space="preserve">
@article{zhou2024calibrated,
  title={Calibrated self-rewarding vision language models},
  author={Zhou, Yiyang and Fan, Zhiyuan and Cheng, Dongjie and Yang, Sihan and Chen, Zhaorun and Cui, Chenhang and Wang, Xiyao and Li, Yun and Zhang, Linjun and Yao, Huaxiu},
  journal={arXiv preprint arXiv:2405.14622},
  year={2024}
}
</pre>
    </div>
  </td>
</tr>




<tr></tr>
  <td width="30%" valign="top" align="center">
    <img src="images/escirl/escirl.jpg" 
         alt="sym" 
         width="90%"
         style="padding-top:0px; padding-bottom:0px; border-radius:15px;">
  </td>
  <td width="60%" valign="top">
    <p><a href="https://openreview.net/pdf?id=1IzW0aniyg" id="ESCIRL">
    <heading>EscIRL: Evolving Self-Contrastive IRL for Trajectory Prediction in Autonomous Driving</heading></a><br>
    Siyue Wang<sup>*</sup>, <b>Zhaorun Chen<sup>*</sup></b>, Zhuokai Zhao, Chaoli Mao, Yiyang Zhou, Jiayu He, Albert Hu<br>
    Conference on Robot Learning (<b>CoRL</b>), 2024<br>
    <!-- short version presented at ICLR 2024 <a href="https://iclr-r2fm.github.io/">R2-FM</a> Workshop<br> -->
    </p>

    <div class="paper" id="escirl">
      <div class="pub-buttons">
        <a href="https://openreview.net/pdf?id=1IzW0aniyg" class="pub-button primary">pdf</a>
        <a href="javascript:toggleblock('escirl_abs')" class="pub-button text">abstract</a>
        <a href="https://openreview.net/pdf?id=1IzW0aniyg" class="pub-button text">arXiv</a>
        <!-- <a href="https://x.com/ZRChen_AISafety/status/1802600945567822117">blog</a>  -->
      </div>

      <p align="justify"> <i id="escirl_abs">While deep neural networks (DNN) and inverse reinforcement learning (IRL) have both been commonly used in autonomous driving to predict trajectories through learning from expert demonstrations, DNN-based methods suffer from data-scarcity, while IRL-based approaches often struggle with generalizability, making both hard to apply to new driving scenarios. To address these issues, we introduce EscIRL, a novel decoupled bi-level training framework that iteratively learns robust reward models from only a few mixed-scenario demonstrations. At the inner level, EscIRL introduces a self-contrastive IRL module that learns a spectrum of specialized reward functions by contrasting demonstrations across different scenarios. At the outer level, ESCIRL employs an evolving loop that iteratively refines the contrastive sets, ensuring global convergence. Experiments on two multi-scenario datasets, CitySim and INTERACTION, demonstrate the effectiveness of EscIRL, outperforming state-of-the-art DNN and IRL-based methods by 41.3% on average. Notably, we show that EscIRL achieves superior generalizability compared to DNN-based approaches while requiring only a small fraction of the data, effectively addressing data-scarcity constraints. All code and data are available at <a href="https://github.com/SiyueWang-CiDi/EscIRL" style="color:#36AE7C;">here</a>.</i></p>

<pre xml:space="preserve">
@inproceedings{wang2024escirl,
  title={EscIRL: Evolving Self-Contrastive IRL for Trajectory Prediction in Autonomous Driving},
  author={Wang, Siyue and Chen, Zhaorun and Zhao, Zhuokai and Mao, Chaoli and Zhou, Yiyang and He, Jiayu and Hu, Albert Sibo},
  booktitle={8th Annual Conference on Robot Learning},
  year={2024}
}
</pre>
    </div>
  </td>
</tr>

<tr>
  <td width="30%" valign="top" align="center">
    <img src="images/autoprm/autoprm.png" 
         alt="sym" 
         width="90%"
         style="padding-top:0px; padding-bottom:0px; border-radius:15px;">
  </td>
  <td width="60%" valign="top">
    <p><a href="https://arxiv.org/abs/2402.11452" id="AUTOPRM">
    <heading>AutoPRM: Automating Procedural Supervision for Multi-Step Reasoning via Controllable Question Decomposition</heading></a><br>
    <b>Zhaorun Chen</b>, Zhuokai Zhao, Zhihong Zhu, Ruiqi Zhang, Xiang Li, Bhiksha Raj and Huaxiu Yao<br>
    North American Chapter of the Association for Computational Linguistics (<b>NAACL</b>), 2024<br>
    <!-- short version presented at ICLR 2024 <a href="https://iclr-r2fm.github.io/">R2-FM</a> Workshop<br> -->
    </p>

    <div class="paper" id="autoprm">
      <div class="pub-buttons">
        <a href="https://arxiv.org/pdf/2402.11452.pdf" class="pub-button primary">pdf</a>
        <a href="javascript:toggleblock('autoprm_abs')" class="pub-button text">abstract</a>
        <a href="https://arxiv.org/abs/2402.11452" class="pub-button text">arXiv</a>
        <a href="https://www.alphaxiv.org/abs/2402.11452" class="pub-button text">alphaxiv</a>
        <a href="https://x.com/ZRChen_AISafety/status/1802600945567822117" class="pub-button text">blog</a> 
      </div>

      <p align="justify"> <i id="autoprm_abs">Recent advancements in large language models (LLMs) have shown promise in multi-step reasoning tasks, yet their reliance on extensive manual labeling to provide procedural feedback remains a significant impediment. To address this challenge, in this paper, we propose a novel self-supervised framework AutoPRM that efficiently enhances the fine-tuning of LLMs for intricate reasoning challenges. Specifically, AutoPRM first decomposes complex problems into more manageable subquestions with a controllable granularity switch, then sequentially apply reinforcement learning to iteratively improve the subquestion solver. Additionally, we propose context-guided-decoding to avoid reward tampering and guide the subquestion solver towards the solution of the holistic problem. Extensive experiments show that AutoPRM significantly improves performance on mathematical and commonsense reasoning tasks over SOTA. More encouragingly, AutoPRM can be easily integrated with other orthogonal reasoning pipelines.</i></p>

<pre xml:space="preserve">
@article{chen2024autoprm,
  title={AutoPRM: Automating Procedural Supervision for Multi-Step Reasoning via Controllable Question Decomposition},
  author={Chen, Zhaorun and Zhao, Zhuokai and Zhu, Zhihong and Zhang, Ruiqi and Li, Xiang and Raj, Bhiksha and Yao, Huaxiu},
  journal={arXiv preprint arXiv:2402.11452},
  year={2024}
}
</pre>
    </div>
  </td>
</tr>


<tr>
  <td width="30%" valign="top" align="center">
    <video playsinline 
           autoplay 
           loop 
           muted 
           src="images/acs/ACS-Video.mp4"
           width="90%"
           style="padding-top:0px; padding-bottom:0px; border-radius:15px;">
    </video>
  </td>
  <td width="60%" valign="top">
    <p><a href="https://arxiv.org/abs/2310.03379" id="ACS">
    <heading>Safe Reinforcement Learning via Hierarchical Adaptive Chance-Constraint Safeguards</heading></a><br>
    <b>Zhaorun Chen</b>, Zhuokai Zhao, Tairan He, Binhao Chen, Xuhao Zhao, Liang Gong, Chengliang Liu<br>
    IEEE/RSJ International Conference on Intelligent Robots and Systems (<b>IROS</b>), 2024<br><span style="color:rgb(246, 54, 54)"><b>(Oral Pitch)</b></span>
    </p>

    <div class="paper" id="acs" style="margin-top: -17px;">
      <!-- <a href="https://manipulation-locomotion.github.io">webpage</a> | -->
      <div class="pub-buttons">
        <a href="https://arxiv.org/pdf/2310.03379.pdf" class="pub-button primary">pdf</a>
        <a href="javascript:toggleblock('acs_abs')" class="pub-button text">abstract</a>
        <a href="https://arxiv.org/abs/2310.03379" class="pub-button text">arXiv</a> 
        <!-- <a href="https://www.alphaxiv.org/abs/2310.03379">alphaxiv</a> | -->
      </div>

      <p align="justify"> <i id="acs_abs">Ensuring safety in Reinforcement Learning (RL), typically framed as a Constrained Markov Decision Process (CMDP), is crucial for real-world exploration applications. Current approaches in handling CMDP struggle to balance optimality and feasibility, as direct optimization methods cannot ensure state-wise in-training safety, and projection-based methods correct actions inefficiently through lengthy iterations. To address these challenges, we propose Adaptive Chance-constrained Safeguards (ACS), an adaptive, model-free safe RL algorithm using the safety recovery rate as a surrogate chance constraint to iteratively ensure safety during exploration and after achieving convergence. Theoretical analysis indicates that the relaxed probabilistic constraint sufficiently guarantees forward invariance to the safe set. And extensive experiments conducted on both simulated and real-world safety-critical tasks demonstrate its effectiveness in enforcing safety (nearly zero-violation) while preserving optimality (+23.8%), robustness, and fast response in stochastic real-world settings.</i></p>

<pre xml:space="preserve">
@misc{chen2024safereinforcementlearninghierarchical,
  title={Safe Reinforcement Learning via Hierarchical Adaptive Chance-Constraint Safeguards}, 
  author={Zhaorun Chen and Zhuokai Zhao and Tairan He and Binhao Chen and Xuhao Zhao and Liang Gong and Chengliang Liu},
  year={2024},
  eprint={2310.03379},
  archivePrefix={arXiv},
  primaryClass={cs.RO},
  url={https://arxiv.org/abs/2310.03379}, 
}
</pre>
    </div>
  </td>
</tr>



<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10" style="margin-top: 25px;">
  <tr><td><sectionheading>&nbsp;&nbsp;Preprints </sectionheading>(see full list on <a href="https://scholar.google.com/citations?user=UZg5N5UAAAAJ">Google Scholar</a>)</td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">
  <!-- Added extra space below preprints heading -->
  <tr><td style="padding-top:5px; padding-bottom:15px;"></td></tr>
  <!-- End of added space -->


<tr>
  <td width="30%" valign="top" align="center">
    <video playsinline 
           autoplay 
           loop 
           muted 
           src="images/grape/grape.mp4"
           width="90%"
           style="padding-top:0px; padding-bottom:0px; border-radius:15px;">
    </video>
  </td>
  <td width="60%" valign="top">
    <p><a href="https://grape-vla.github.io/" id="SafeWatch">
    <heading>GRAPE: Generalizing Robot Policy via Preference Alignment</heading></a><br>
    Zijian Zhang<sup>*</sup>, Kaiyuan Zheng<sup>*</sup>, <b>Zhaorun Chen<sup>*</sup></b>, Joel Jang, Yi Li, Chaoqi Wang, Mingyu Ding, Dieter Fox, and Huaxiu Yao<br>
    In submission, 2025<br>
    </p>

    <div class="paper" id="grape">
      <div class="pub-buttons">
        <a href="https://arxiv.org/pdf/2411.19309" class="pub-button primary">pdf</a>
        <a href="https://grape-vla.github.io/" class="pub-button secondary">webpage</a>
        <a href="javascript:toggleblock('grape_abs')" class="pub-button text">abstract</a>
        <a href="https://arxiv.org/pdf/2411.19309" class="pub-button text">arXiv</a>
        <a href="https://www.alphaxiv.org/abs/2411.19309" class="pub-button text">alphaxiv</a>
        <a href="https://x.com/HuaxiuYaoML/status/1863708749732680162" class="pub-button text">blog</a> 
      </div>

      <p align="justify"> <i id="grape_abs">Despite the recent advancements of vision-language-action (VLA) models on a variety of robotics tasks, they suffer from critical issues such as poor generalizability to unseen tasks, due to their reliance on behavior cloning exclusively from successful rollouts. Furthermore, they are typically fine-tuned to replicate demonstrations collected by experts under different settings, thus introducing distribution bias and limiting their adaptability to diverse manipulation objectives, such as efficiency, safety, and task completion. To bridge this gap, we introduce GRAPE: Generalizing Robot Policy via Preference Alignment. Specifically, GRAPE aligns VLAs on a trajectory level and implicitly models reward from both successful and failure trials to boost generalizability to diverse tasks. Moreover, GRAPE breaks down complex manipulation tasks to independent stages and automatically guides preference modeling through customized spatiotemporal constraints with keypoints proposed by a large vision-language model. Notably, these constraints are flexible and can be customized to align the model with varying objectives, such as safety, efficiency, or task success. We evaluate GRAPE across a diverse array of tasks in both real-world and simulated environments. Experimental results demonstrate that GRAPE enhances the performance of state-of-the-art VLA models, increasing success rates on in-domain and unseen manipulation tasks by 51.79% and 60.36%, respectively. Additionally, GRAPE can be aligned with various objectives, such as safety and efficiency, reducing collision rates by 44.31% and rollout step-length by 11.15%, respectively. All code, models, and data are available at <a href="https://grape-vla.github.io/" style="color:#36AE7C;">here</a>.</i></p>

<pre xml:space="preserve">
@article{zhang2024grape,
  title={Grape: Generalizing robot policy via preference alignment},
  author={Zhang, Zijian and Zheng, Kaiyuan and Chen, Zhaorun and Jang, Joel and Li, Yi and Wang, Chaoqi and Ding, Mingyu and Fox, Dieter and Yao, Huaxiu},
  journal={arXiv preprint arXiv:2411.19309},
  year={2024}
}
</pre>
    </div>
  </td>
</tr>


</table>


<hr style="height: 2px; background-color: #800000; margin-top: 30px; margin-bottom: 20px;"/>

<!-- Add these sections right before your footer section (the final hr) -->

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Organizer</sectionheading></td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">
  <tr>
    <td style="padding:20px;width:100%;vertical-align:middle">
      <p>
      <a href="https://www.llmagentsafetycomp24.com/">The LLM and Agent Safety Competition (CLAS 2024)</a>, NeurIPS 2024<br>
      <a href="https://sites.google.com/view/aia-workshop/">COLM 2025 Workshop on AI Agents: Capabilities and Safety (AIA 2025)</a>, COLM 2025
      </p>
    </td>
  </tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Reviewer Service</sectionheading></td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">
  <tr>
    <td style="padding:20px;width:100%;vertical-align:middle">
      <b>Area Chair:</b> EMNLP'25<br>
      <b>Conference Reviewer:</b> NeurIPS'25, ICML'25, ICCV'25, CVPR'25, NeurIPS'24, ICLR'24'25, COLM'24, ARR'24, IROS'24
    </td>
  </tr>
</table>

<hr style="height: 2px; background-color: #800000; margin-top: 30px; margin-bottom: 20px;"/>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="2">
    <tr><td><br><p align="right" style="color: #800000;"><font size="1.5">
    University of Chicago | Department of Computer Science<br>
    Template modified from <a href="http://www.cs.berkeley.edu/~barron/" style="color: #800000;">Jon Barron</a>
    </font></p></td></tr>
</table>

  </td></tr>
</table>


<script xml:space="preserve" language="JavaScript">
  hideallbibs();
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('material_review_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('ieee_iot_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('mjvideo_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('fisao_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('mmie_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('mmdt_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('anyprefer_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('safewatch_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('grape_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('agentpoison_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('csr_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('mjbench_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('escirl_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('halc_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('autoprm_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('acs_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('shieldagent_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('guardsetxabs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('autoredteamerabs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('dropoutdecodingabs');
  </script>
  <script>
  document.addEventListener("DOMContentLoaded", function() {
    const lazyMedia = document.querySelectorAll(".lazy-media");
    
    const lazyLoadMedia = (entries, observer) => {
      entries.forEach(entry => {
        if (entry.isIntersecting) {
          const media = entry.target;
          media.src = media.dataset.src;
          media.classList.add("loaded");
          observer.unobserve(media);
        }
      });
    };
  
    const mediaObserver = new IntersectionObserver(lazyLoadMedia, {
      root: null,
      threshold: 0.1
    });
  
    lazyMedia.forEach(media => mediaObserver.observe(media));
  });
</script>
</body>

</html>