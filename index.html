
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* Design Modified from: Deepak Pathak, Jon Barron, and Saurabh Gupta. */
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 400
  }
  heading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 17px; /* 19 */
    font-weight: 600 /* 1000 */
  }
  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }
  strong {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 600 /* 800 */
  }
  strongred {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    color: 'red' ;
    font-size: 16px
  }
  sectionheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    font-weight: 600
  }
  pageheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 38px;
    font-weight: 400
  }
  .center {
    display: block;
    margin-left: auto;
    margin-right: auto;
    width: 80%;
  }
  .ImageBorder
  {
      border-width: 1px;
      border-color: Black;
  }
  span.highlight {
  background-color: #ffffd0;
  }
  </style>
  <link rel="shortcut icon" href="images/favicon.ico" type="image/vnd.microsoft.icon">
  <script type="text/javascript" src="js/hidebib.js"></script>
  <title>Zhaorun Chen, University of Chicago</title>
  <meta name="Zhaorun Chen's Homepage" http-equiv="Content-Type" content="Zhaorun Chen's Homepage">
  <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
</head>

<body>
<table width="1000" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr><td>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <p align="center">
    <pageheading>Zhaorun Chen</pageheading><br>
    <b>email</b>:&nbsp zhaorun (at) uchicago (dot) edu<br>
    <b>office</b>:&nbsp JCL 287, 5801 S Ellis Ave, Chicago, IL 60637
  </p>

  <tr>
    <td width="23%" valign="top" align="center"><a href="images/personal/profile.jpg"><img src="images/personal/profile.jpg" width="90%" style="border-radius:15px"></a>
    <p align=center>
    | <a href="data/CV_zhaorun.pdf">CV</a> |
    <a href="mailto:zhaorun@uchicago.edu">Email</a> |
    <a href="https://scholar.google.com/citations?user=UZg5N5UAAAAJ">Google Scholar</a> |
    <br/>
    | <a href="https://github.com/BillChan226">Github</a> | 
    <a href="https://www.linkedin.com/in/zhaorun-chen-1793b6226/">LinkedIn</a> |
    <!-- <a href="https://space.bilibili.com/14145636">Bilibili</a> |  -->
    </p>
    <p align="center" style="margin-top:-8px;"><iframe id="twitter-widget-0" scrolling="no" frameborder="0" allowtransparency="true" allowfullscreen="true" class="twitter-follow-button twitter-follow-button-rendered" style="position: static; visibility: visible; width: 186px; height: 20px;" title="Twitter Follow Button" src="https://platform.twitter.com/widgets/follow_button.2f70fb173b9000da126c79afe2098f02.en.html#dnt=false&amp;id=twitter-widget-0&amp;lang=en&amp;screen_name=ZRChen_AISafety&amp;show_count=false&amp;show_screen_name=true&amp;size=m&amp;time=1706734206165" data-screen-name=""></iframe><script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>
    </td>
    <td width="70%" valign="top" align="justify">
      <p>I am a first-year Ph.D. student in the <a href="https://aisecure.github.io/">Secure Learning Lab</a> at the <a href="https://cs.uchicago.edu/">Department of Computer Science</a> at <a href="https://www.uchicago.edu/">University of Chicago</a> advised by Prof.<a href="https://aisecure.github.io/"> Bo Li</a>.
      </p>
      <!-- <p>Previously, I received my Master degree in <a href="https://engineering.purdue.edu/ECE">Electrical and Computer Engineering</a> at<a href="https://www.purdue.edu/"> Purdue University</a> advised by Prof.<a href="https://engineering.purdue.edu/~lusu/"> Su Lu</a>. Before that, I obtained my Bachelor degree in Automation at <a href="http://en.sjtu.edu.cn"> Shanghai Jiao Tong University</a>, advised by Prof.<a href="https://gaoyue.sjtu.edu.cn"> Yue Gao</a>. -->
        <p>Previously, I received my Master degree in <a href="https://engineering.purdue.edu/ECE">Electrical and Computer Engineering</a> at<a href="https://www.purdue.edu/"> Purdue University</a> and my Bachelor degree in Automation at <a href="http://en.sjtu.edu.cn"> Shanghai Jiao Tong University</a>. I also work closely with Prof. <a href="https://www.huaxiuyao.io/">Huaxiu Yao</a> at UNC-Chapel Hill and Prof. <a href="https://dawnsong.io/">Dawn Song</a> at UC Berkeley.
        <!-- During 2023 Summer, I interned at <a href="https://cs.unc.edu"> UNC-Chapel Hill</a> advised by Prof.<a href="https://www.huaxiuyao.io/"> Huaxiu Yao</a> and collaborated some wonderful projects with <a href="https://irislab.stanford.edu/people.html"> IRIS Lab</a> hosted by Prof. <a href="https://ai.stanford.edu/~cbfinn/"> Chelsea Finn</a>. -->
      </p>
      <p>My current research interests center on the trustworthy and alignment issue of foundation models (e.g. LLMs) and agents from both a theoretical and application perspective. Specifically, I’m interested in enhancing their trustworthiness via novel algorithms and certificates for various applications (e.g. <b>hallucination mitigation</b>, <b>training & testing-time attack</b>, <b>guardrail models</b>) through incorporating external knowledge sources and LLMs’ reasoning capabilities.
      </p>
      <p><a href="https://billchan226.github.io/publication.html">[Publications]</a> Email: zhaorun [AT] uchicago.edu
      </p>
    </td>
  </tr>
</table>

<!-- <hr/> -->
<hr/>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
  <tr>
  <td style="padding:2px;width:100%;vertical-align:middle">
    <heading>News</heading>
    <p>
      <ul>
        <!-- <li><b style="color: #2196F3;">[02/2025]</b> 🔥 We have initiated the <a href="https://huggingface.co/MJ-Bench" style="color: #03f59c;">MJ-Bench-Team</a> to build better and more trustworthy foundation models!</li>           -->
        <li><b style="color: #2196F3;">[02/2025]</b> 🔥 I will join the Multimodal AI team at <a href="https://ai.meta.com/" style="color: #2196F3;">GenAI@Meta</a> as a research scientist intern in summer 2025!</li>
        <li><b style="color: #2196F3;">[01/2025]</b> 🎉 Five papers accepted by <a href="https://iclr.cc/" style="color: #a200ff;">ICLR 2025</a>, including two first-authored papers and one oral paper!</li>  
        <!-- <li><b style="color: #2196F3;">[Aug, 2024]</b> 🎉 One first-authored paper accepted by <a href="https://www.corl.org/" style="color: #a200ff;">CoRL 2024</a>!</li>   -->
        <li><b style="color: #2196F3;">[09/2024]</b> 🎉 Two papers accepted by <a href="https://neurips.cc/" style="color: #ff5900;">NeurIPS 2024</a>!</li>
        <!-- <li><b style="color: #2196F3;">[07/2024]</b> 🌟 We are hosting the <a href="https://www.llmagentsafetycomp24.com/" style="color: #f00741;">The LLM and Agent Safety Competition (CLAS 2024)</a> at <a href="https://neurips.cc/" style="color: #f00741;">NeurIPS 2024</a>, stay tuned!</li> -->
          <!-- <li><b style="color: #2196F3;">[June, 2024]</b> 🏆 One first-authored <a href="https://arxiv.org/pdf/2310.03379" style="color: #ff7300;">paper</a> is accepted by <a href="https://iros2024-abudhabi.org/" style="color: #ff7300;">IROS 2024</a> for <strong>Oral</strong> presentation!</li> -->
          <!-- <li><b style="color: #2196F3;">[June, 2024]</b> 🏆 One paper accepted to <a href="https://2024.acmmm.org/" style="color: #03f59c;">ACM MM 2024</a> for <strong>Oral</strong> presentation!</li> -->
          <li><b style="color: #2196F3;">[05/2024]</b> 🎉 One first-authored <a href="https://arxiv.org/pdf/2403.00425.pdf" style="color: #2b00ff;">paper</a> accepted by <a href="https://icml.cc/" style="color: #2b00ff;">ICML 2024</a>!</li>
          <li><b style="color: #2196F3;">[03/2024]</b> 🌟 One first-authored <a href="https://arxiv.org/abs/2402.11452" style="color: #9C27B0;">paper</a> accepted by <a href="https://2024.naacl.org/" style="color: #9C27B0;">NAACL 2024</a>!</li>
          <!-- <li><b style="color: #2196F3;">[Feb., 2024]</b> 🎓 Four short-version papers presented at ICLR 2024 Workshops!</li> -->
          <li><b style="color: #2196F3;">[02/2024]</b> 🎉 I've joined <a href="https://aisecure.github.io/" style="color: #FF9800;">Secure Learning Lab</a> at UChicago as a Ph.D. student advised by Prof. <a href="https://aisecure.github.io/" style="color: #FF9800;">Bo Li</a>.</li>
          <!-- <li><b style="color: #2196F3;">[May, 2023]</b> 📜 One paper accepted by <a href="https://www.usenix.org/conference/usenixsecurity23" style="color: #673AB7;">USENIX Security 2023</a>.</li>
          <li><b style="color: #2196F3;">[Feb, 2023]</b> 🏆 I am selected as one of the CS candidates for Conference Presentation Awards for Graduate Students!</li>
          <li><b style="color: #2196F3;">[Nov, 2022]</b> 🌟 One paper accepted by <a href="https://satml.org/" style="color: #9C27B0;">IEEE SatML 2023</a>.</li> -->
          <!-- <li><b style="color: #2196F3;">[June, 2023]</b> 🚀 I've joined Prof. <a href="https://www.huaxiuyao.io/" style="color: #CDDC39;">Huaxiu Yao</a> at <a href="https://cs.unc.edu" style="color: #CDDC39;">CS Department at UNC-Chapel Hill</a> as a research intern.</li> -->
          <!-- <li><b style="color: #2196F3;">[Nov., 2021]</b> 📚 First-authored paper received <a href="https://ieeexplore.ieee.org/abstract/document/9597110" style="color: #795548;">Best Paper award</a> at <a href="https://ieeexplore.ieee.org/xpl/conhome/9596614/proceeding" style="color: #795548;">ISRIMT 2021</a>.</li> -->
          <!-- <li><b style="color: #2196F3;">[May, 2021]</b> 🎓 One paper accepted by <a href="https://icml.cc/Conferences/2021" style="color: #9E9E9E;">ICML 2021</a>.</li> -->
      </ul>
    </p>
  </td>
</tr>
</table>



<hr/>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Selected publications </sectionheading>(see full list on <a href="https://scholar.google.com/citations?user=UZg5N5UAAAAJ">Google Scholar</a>)</td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">
<br/>

<tr>
  <td width="30%" valign="top" align="center">
    <video playsinline 
           autoplay 
           loop 
           muted 
           src="images/safewatch/website_demonstration.mov"
           width="90%"
           style="padding-top:0px; padding-bottom:0px; border-radius:15px;">
    </video>
  </td>
  <td width="60%" valign="top">
    <p><a href="https://safewatch-aiguard.github.io/" id="SafeWatch">
    <heading>SafeWatch: An Efficient Safety-Policy Following Video Guardrail Model with Transparent Explanations</heading></a><br>
    <b>Zhaorun Chen</b>, Francesco Pinto, Minzhou Pan, and Bo Li<br>
    International Conference on Learning Representations (<b>ICLR</b>), 2025<br>
    </p>

    <div class="paper" id="safewatch">
    <a href="https://arxiv.org/pdf/2412.06878">pdf</a> |
    <a href="https://safewatch-aiguard.github.io/">webpage</a> |
    <a href="javascript:toggleblock('safewatch_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('safewatch')" class="togglebib">bibtex</a> |
    <a href="https://arxiv.org/abs/2412.06878">arXiv</a> |
    <a href="https://www.alphaxiv.org/abs/2412.06878">alphaxiv</a> |
    <a href="https://x.com/ZRChen_AISafety/status/1869621962223616130">blog</a> 

    <p align="justify"> <i id="safewatch_abs">With the rise of generative AI and rapid growth of high-quality video generation, video guardrails have become more crucial than ever to ensure safety and security across platforms. Current video guardrails, however, are either overly simplistic, relying on pure classification models trained on simple policies with limited unsafe categories, which lack detailed explanations, or prompting multimodal large language models (MLLMs) with long safety guidelines, which are inefficient and impractical for guardrailing real-world content. To bridge this gap, we propose SafeWatch, an efficient MLLM-based video guardrail model designed to follow customized safety policies and provide multi-label video guardrail outputs with content-specific explanations in a zero-shot manner. In particular, unlike traditional MLLM-based guardrails that encode all safety policies autoregressively, causing inefficiency and bias, SafeWatch uniquely encodes each policy chunk in parallel and eliminates their position bias such that all policies are attended simultaneously with equal importance. In addition, to improve efficiency and accuracy, SafeWatch incorporates a policy-aware visual token pruning algorithm that adaptively selects the most relevant video tokens for each policy, discarding noisy or irrelevant information. This allows for more focused, policy-compliant guardrail with significantly reduced computational overhead. Considering the limitations of existing video guardrail benchmarks, we propose SafeWatch-Bench, a large-scale video guardrail benchmark comprising over 2M videos spanning six safety categories which covers over 30 tasks to ensure a comprehensive coverage of all potential safety scenarios. We have conducted extensive experiments, showing that SafeWatch outperforms all SOTA video guardrails on SafeWatch-Bench by 28.2%, and achieves a 13.6% improvement on existing benchmarks, all while reducing inference costs by an average of 10%. SafeWatch also demonstrates strong policy-following abilities and outperforms previous SOTAs by 5.6% and 15.6% in zero-shot generalizability to new policies and new prompting tasks. Additionally, both LLM-as-a-judge and human evaluators confirm the high quality of the explanations provided by SafeWatch. Our project is open-sourced at <a href="https://safewatch-aiguard.github.io" style="color:#36AE7C;">here</a>.</i></p>

<pre xml:space="preserve">
@article{chen2024safewatch,
  title={SafeWatch: An Efficient Safety-Policy Following Video Guardrail Model with Transparent Explanations},
  author={Chen, Zhaorun and Pinto, Francesco and Pan, Minzhou and Li, Bo},
  journal={arXiv preprint arXiv:2412.06878},
  year={2024}
}
</pre>
    </div>
  </td>
</tr>


<tr>
  <td width="30%" valign="top" align="center">
    <img src="images/mmdt/mmdt.png" 
         alt="sym" 
         width="90%"
         style="padding-top:0px; padding-bottom:0px; border-radius:15px;">
  </td>
  <td width="60%" valign="top">
    <p><a href="https://mmdecodingtrust.github.io/" id="MMDT">
    <heading>MMDT: Decoding the Trustworthiness and Safety of Multimodal Foundation Models</heading></a><br>
    Chejian Xu<sup>*</sup>, Jiawei Zhang<sup>*</sup>, <b>Zhaorun Chen<sup>*</sup></b>, Chulin Xie<sup>*</sup>, Mintong Kang<sup>*</sup>, Yujin Potter<sup>*</sup>, Zhun Wang<sup>*</sup>, Zhuowen Yuan<sup>*</sup>, Alexander Xiong, Zidi Xiong, Chenhui Zhang, Lingzhi Yuan, Yi Zeng, Peiyang Xu, Chengquan Guo, Andy Zhou, Jeffrey Ziwei Tan, Xuandong Zhao, Francesco Pinto, Zhen Xiang, Yu Gai, Zinan Lin, Dan Hendrycks, Bo Li, Dawn Song<br>
    International Conference on Learning Representations (<b>ICLR</b>), 2025<br>
    </p>

    <div class="paper" id="mmdt">
    <a href="">pdf</a> |
    <a href="https://mmdecodingtrust.github.io/">webpage</a> |
    <a href="javascript:toggleblock('mmdt_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('mmdt')" class="togglebib">bibtex</a> |
    <a href="">arXiv</a> |
    <!-- <a href="">alphaxiv</a> | -->
    <!-- <a href="https://x.com/ZRChen_AISafety/status/1814412843904840095">blog</a> -->

    <p align="justify"> <i id="mmdt_abs">Multimodal foundation models (MMFMs) play a crucial role in various applications, including autonomous driving, healthcare, and virtual assistants. However, several studies have revealed vulnerabilities in these models, such as generating unsafe content by text-to-image models. Existing benchmarks on multimodal models either predominantly assess the helpfulness of these models, or only focus on limited perspectives such as fairness and privacy. In this paper, we present the first unified platform, MMDT (Multimodal DecodingTrust), designed to provide a comprehensive safety and trustworthiness evaluation for MMFMs. Our platform assesses models from multiple perspectives, including safety, hallucination, fairness/bias, privacy, adversarial robustness, and out-of-distribution (OOD) generalization. We have designed various evaluation scenarios and red teaming algorithms under different tasks for each perspective to generate challenging data, forming a high-quality benchmark. We evaluate a range of multimodal models using MMDT, and our findings reveal a series of vulnerabilities and areas for improvement across these perspectives. This work introduces the first comprehensive and unique safety and trustworthiness evaluation platform for MMFMs, paving the way for developing safer and more reliable MMFMs and systems.</i></p>

<pre xml:space="preserve">
@inproceedings{
  xu2025mmdt,
  title={{MMDT}: Decoding the Trustworthiness and Safety of Multimodal Foundation Models},
  author={Chejian Xu and Jiawei Zhang and Zhaorun Chen and Chulin Xie and Mintong Kang and Zhuowen Yuan and Zidi Xiong and Chenhui Zhang and Lingzhi Yuan and Yi Zeng and Peiyang Xu and Chengquan Guo and Andy Zhou and Jeffrey Ziwei Tan and Zhun Wang and Alexander Xiong and Xuandong Zhao and Yu Gai and Francesco Pinto and Yujin Potter and Zhen Xiang and Zinan Lin and Dan Hendrycks and Dawn Song and Bo Li},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2025},
  url={https://openreview.net/forum?id=qIbbBSzH6n}
  }
</pre>
    </div>
  </td>
</tr>

<tr></tr>
  <td width="30%" valign="top" align="center">
    <img src="images/mmie/mmie.jpg" 
         alt="sym" 
         width="90%"
         style="padding-top:0px; padding-bottom:0px; border-radius:15px;">
  </td>
  <td width="60%" valign="top">
    <p><a href="https://mmie-bench.github.io/" id="MMIE">
    <heading>MMIE: Massive Multimodal Interleaved Comprehension Benchmark For Large Vision-Language Models</heading></a><br>
    Peng Xia<sup>*</sup>, Siwei Han<sup>*</sup>, Shi Qiu<sup>*</sup>, Yiyang Zhou, Zhaoyang Wang, Wenhao Zheng, <b>Zhaorun Chen</b>, Chenhang Cui, Mingyu Ding, Linjie Li, Lijuan Wang, Huaxiu Yao<br>
    International Conference on Learning Representations (<b>ICLR</b>), 2025 <br><span style="color:red;"><b>(Oral Presentation)</b></span>
    </p>

    <div class="paper" id="mmie" style="margin-top: -17px;">
    <a href="https://arxiv.org/pdf/2410.10139">pdf</a> |
    <a href="https://mmie-bench.github.io/">webpage</a> |
    <a href="javascript:toggleblock('mmie_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('mmie')" class="togglebib">bibtex</a> |
    <a href="https://arxiv.org/pdf/2410.10139">arXiv</a> |
    <a href="https://www.alphaxiv.org/abs/2410.10139">alphaxiv</a> |
    <a href="https://x.com/HuaxiuYaoML/status/1846014867129434259">blog</a>

    <p align="justify"> <i id="mmie_abs">Interleaved multimodal comprehension and generation, enabling models to produce and interpret both images and text in arbitrary sequences, have become a pivotal area in multimodal learning. Despite significant advancements, the evaluation of this capability remains insufficient. Existing benchmarks suffer from limitations in data scale, scope, and evaluation depth, while current evaluation metrics are often costly or biased, lacking in reliability for practical applications. To address these challenges, we introduce MMIE, a large-scale knowledge-intensive benchmark for evaluating interleaved multimodal comprehension and generation in Large Vision-Language Models (LVLMs). MMIE comprises 20K meticulously curated multimodal queries, spanning 3 categories, 12 fields, and 102 subfields, including mathematics, coding, physics, literature, health, and arts. It supports both interleaved inputs and outputs, offering a mix of multiple-choice and open-ended question formats to evaluate diverse competencies. Moreover, we propose a reliable automated evaluation metric, leveraging a scoring model fine-tuned with human-annotated data and systematic evaluation criteria, aimed at reducing bias and improving evaluation accuracy. Extensive experiments demonstrate the effectiveness of our benchmark and metrics in providing a comprehensive evaluation of interleaved LVLMs. Specifically, we evaluate eight LVLMs, revealing that even the best models show significant room for improvement, with most achieving only moderate results. We believe MMIE will drive further advancements in the development of interleaved LVLMs.</i></p>

<pre xml:space="preserve">
@article{xia2024mmie,
  title={MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models},
  author={Xia, Peng and Han, Siwei and Qiu, Shi and Zhou, Yiyang and Wang, Zhaoyang and Zheng, Wenhao and Chen, Zhaorun and Cui, Chenhang and Ding, Mingyu and Li, Linjie and Wang, Lijuan and Yao, Huaxiu},
  journal={arXiv preprint arXiv:2410.10139},
  year={2024}
}
</pre>
    </div>
  </td>
</tr>


<tr>
  <td width="30%" valign="top" align="center">
    <img src="images/agentpoison/method.png" 
         alt="sym" 
         width="90%"
         style="padding-top:0px; padding-bottom:0px; border-radius:15px;">
  </td>
  <td width="60%" valign="top">
    <p><a href="https://billchan226.github.io/AgentPoison.html" id="AgentPoison">
    <heading>AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases</heading></a><br>
    <b>Zhaorun Chen</b>, Zhen Xiang, Chaowei Xiao, Dawn Song, and Bo Li<br>
    Advances in Neural Information Processing Systems (<b>NeurIPS</b>), 2024<br>
    </p>

    <div class="paper" id="agentpoison">
    <a href="https://arxiv.org/pdf/2407.12784">pdf</a> |
    <a href="https://billchan226.github.io/AgentPoison.html">webpage</a> |
    <a href="javascript:toggleblock('agentpoison_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('agentpoison')" class="togglebib">bibtex</a> |
    <a href="https://arxiv.org/abs/2310.03379">arXiv</a> |
    <a href="https://www.alphaxiv.org/abs/2407.12784">alphaxiv</a> |
    <a href="https://x.com/ZRChen_AISafety/status/1814412843904840095">blog</a>

    <p align="justify"> <i id="agentpoison_abs">LLM agents have demonstrated remarkable performance across various applications, primarily due to their advanced capabilities in reasoning, utilizing external knowledge and tools, calling APIs, and executing actions to interact with environments. Current agents typically utilize a memory module or a retrieval-augmented generation (RAG) mechanism, retrieving past knowledge and instances with similar embeddings from knowledge bases to inform task planning and execution. However, the reliance on unverified knowledge bases raises significant concerns about their safety and trustworthiness. To uncover such vulnerabilities, we propose a novel red teaming approach AgentPoison, the first backdoor attack targeting generic and RAG-based LLM agents by poisoning their long-term memory or RAG knowledge base. In particular, we form the trigger generation process as a constrained optimization to optimize backdoor triggers by mapping the triggered instances to a unique embedding space, so as to ensure that whenever a user instruction contains the optimized backdoor trigger, the malicious demonstrations are retrieved from the poisoned memory or knowledge base with high probability. In the meantime, benign instructions without the trigger will still maintain normal performance. Unlike conventional backdoor attacks, AgentPoison requires no additional model training or fine-tuning, and the optimized backdoor trigger exhibits superior transferability, in-context coherence, and stealthiness. Extensive experiments demonstrate AgentPoison effectiveness in attacking three types of real-world LLM agents: RAG-based autonomous driving agent, knowledge-intensive QA agent, and healthcare EHRAgent. We inject the poisoning instances into the RAG knowledge base and long-term memories of these agents, respectively, demonstrating the generalization of AgentPoison. On each agent, AgentPoison achieves an average attack success rate of ≥ 80% with minimal impact on benign performance (≤ 1%) with a poison rate < 0.1%. Code is released <a href="https://github.com/BillChan226/AgentPoison" style="color:#36AE7C;">here</a>.</i></p>

<pre xml:space="preserve">
@article{chen2024agentpoison,
  title={AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases},
  author={Chen, Zhaorun and Xiang, Zhen and Xiao, Chaowei and Song, Dawn and Li, Bo},
  journal={arXiv preprint arXiv:2407.12784},
  year={2024}
}
</pre>
    </div>
  </td>
</tr>

<tr>
  <td width="30%" valign="top" align="center">
    <img src="images/halc/halc.png" 
         alt="sym" 
         width="90%"
         style="padding-top:0px; padding-bottom:0px; border-radius:15px;">
  </td>
  <td width="60%" valign="top">
    <p><a href="https://billchan226.github.io/HALC.html" id="HALC">
    <heading>HALC: Object Hallucination Reduction via Adaptive Focal-Contrast Decoding</heading></a><br>
    <b>Zhaorun Chen<sup>*</sup></b>, Zhuokai Zhao<sup>*</sup>, Hongyin Luo, Huaxiu Yao, Bo Li and Jiawei Zhou<br>
    International Conference on Machine Learning (<b>ICML</b>), 2024<br>
    <!-- short version presented at ICLR 2024 <a href="https://iclr-r2fm.github.io/">R2-FM</a> Workshop<br> -->
    </p>

    <div class="paper" id="halc">
    <a href="https://arxiv.org/pdf/2403.00425">pdf</a> |
    <a href="https://billchan226.github.io/HALC.html">webpage</a> |
    <a href="javascript:toggleblock('halc_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('halc')" class="togglebib">bibtex</a> |
    <a href="https://arxiv.org/abs/2310.03379">arXiv</a> |
    <a href="https://www.alphaxiv.org/abs/2403.00425">alphaxiv</a> |
    <a href="https://www.virtueai.com/2024/07/29/halc-object-hallucination-reduction-via-adaptive-focal-contrast-decoding/">blog</a> 

    <p align="justify"> <i id="halc_abs">While large vision-language models (LVLMs) have demonstrated impressive capabilities in interpreting multi-modal contexts, they invariably suffer from object hallucinations (OH). We introduce HALC, a novel decoding algorithm designed to mitigate OH in LVLMs. HALC leverages distinct fine-grained optimal visual information in vision-language tasks and operates on both local and global contexts simultaneously. Specifically, HALC integrates a robust auto-focal grounding mechanism (locally) to correct hallucinated tokens on the fly, and a specialized beam search algorithm (globally) to significantly reduce OH while preserving text generation quality. Additionally, HALC can be integrated into any LVLMs as a plug-and-play module without extra training. Extensive experimental studies demonstrate HALC’s effectiveness in reducing OH, outperforming state-of-the-arts across four benchmarks. Code is released <a href="https://github.com/BillChan226/HALC">here</a>.</i></p>

<pre xml:space="preserve">
@article{chen2024halc,
  title={HALC: Object Hallucination Reduction via Adaptive Focal-Contrast Decoding},
  author={Chen, Zhaorun and Zhao, Zhuokai and Luo, Hongyin and Yao, Huaxiu and Li, Bo and Zhou, Jiawei},
  journal={arXiv preprint arXiv:2403.00425},
  year={2024}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="30%" valign="top" align="center">
    <img src="images/fisao/fisao.jpg" 
         alt="sym" 
         width="90%"
         style="padding-top:0px; padding-bottom:0px; border-radius:15px;">
  </td>
  <td width="60%" valign="top">
    <p><a href="" id="FineGrainedVerifiers">
    <heading>Fine-Grained Verifiers: Preference Modeling as Next-token Prediction in Vision-Language Alignment</heading></a><br>
    Chenhang Cui<sup>*</sup>, An Zhang<sup>*</sup>, Yiyang Zhou, <b>Zhaorun Chen</b>, Gelei Deng, Huaxiu Yao, Tat-Seng Chua<br>
    International Conference on Learning Representations (<b>ICLR</b>), 2025<br>
    </p>

    <div class="paper" id="anyprefer">
    <a href="https://arxiv.org/pdf/2410.14148">pdf</a> |
    <a href="">webpage</a> |
    <a href="javascript:toggleblock('fisao_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('fisao')" class="togglebib">bibtex</a> |
    <a href="https://arxiv.org/pdf/2410.14148">arXiv</a> |
    <a href="https://www.alphaxiv.org/abs/2410.14148">alphaxiv</a> |
    <!-- <a href="https://x.com/HuaxiuYaoML/status/1846014867129434259">blog</a> -->

    <p align="justify"> <i id="fisao_abs">The recent advancements in large language models (LLMs) and pre-trained vision models have accelerated the development of vision-language large models (VLLMs), enhancing the interaction between visual and linguistic modalities. Despite their notable success across various domains, VLLMs face challenges in modality alignment, which can lead to issues like hallucinations and unsafe content generation. Current alignment techniques often rely on coarse feedback and external datasets, limiting scalability and performance. In this paper, we propose FiSAO (Fine-Grained Self-Alignment Optimization), a novel self-alignment method that utilizes the model’s own visual encoder as a fine-grained verifier to improve vision-language alignment without the need for additional data. By leveraging token-level feedback from the vision encoder, FiSAO significantly improves vision-language alignment, even surpassing traditional preference tuning methods that require additional data. Through both theoretical analysis and experimental validation, we demonstrate that FiSAO effectively addresses the misalignment problem in VLLMs, marking the first instance of token-level rewards being applied to such models.</i></p>

<pre xml:space="preserve">
@article{cui2024fine,
  title={Fine-Grained Verifiers: Preference Modeling as Next-token Prediction in Vision-Language Alignment},
  author={Cui, Chenhang and Zhang, An and Zhou, Yiyang and Chen, Zhaorun and Deng, Gelei and Yao, Huaxiu and Chua, Tat-Seng},
  journal={arXiv preprint arXiv:2410.14148},
  year={2024}
}
</pre>
    </div>
  </td>
</tr>


<tr>
  <td width="30%" valign="top" align="center">
    <img src="images/anyprefer/anyprefer.jpg" 
         alt="sym" 
         width="90%"
         style="padding-top:0px; padding-bottom:0px; border-radius:15px;">
  </td>
  <td width="60%" valign="top">
    <p><a href="" id="AnyPrefer">
    <heading>Anyprefer: An Agentic Framework for Preference Data Synthesis</heading></a><br>
    Yiyang Zhou, Zhaoyang Wang, Tianle Wang, Shangyu Xing, Peng Xia, Bo Li, Kaiyuan Zheng, Zijian Zhang, <b>Zhaorun Chen</b>, Wenhao Zheng, Xuchao Zhang, Chetan Bansal, Weitong Zhang, Ying Wei, Mohit Bansal, Huaxiu Yao<br>
    International Conference on Learning Representations (<b>ICLR</b>), 2025<br>
    </p>

    <div class="paper" id="anyprefer">
    <a href="">pdf</a> |
    <a href="">webpage</a> |
    <a href="javascript:toggleblock('anyprefer_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('anyprefer')" class="togglebib">bibtex</a> |
    <a href="">arXiv</a> |
    <!-- <a href="">alphaxiv</a> | -->
    <!-- <a href="https://x.com/HuaxiuYaoML/status/1846014867129434259">blog</a> -->

    <p align="justify"> <i id="anyprefer_abs">High-quality preference data is essential for aligning foundation models with human values through preference learning. However, manual annotation of such data is often time-consuming and costly. Recent methods adopt a self-rewarding approach, where the target model generates and annotates its own preference data, but this can lead to inaccuracies due to the reward model sharing weights with the target model, amplifying inherent biases. To address these issues, we propose Anyprefer, a framework designed to synthesize high-quality preference data for the target model. Anyprefer frames the data synthesis process as a cooperative two-player Markov Game, where the target model and a judge model collaborate. Here, a series of external tools are introduced to assist the judge model in accurately rewarding the target model’s responses, mitigating biases in the process. We also introduce a feedback mechanism to optimize prompts for both models, enhancing collaboration and improving data quality. The synthesized data is compiled into a new preference dataset, Anyprefer-V1, consisting of 58K high-quality preference pairs. Extensive experiments show that Anyprefer significantly improves model alignment across four applications, covering 21 datasets, achieving average improvements of 18.55 in five natural language generation datasets, 3.66 in nine vision-language understanding datasets, 30.05 in three medical image analysis datasets, and 14.50 in four visuo-motor control tasks.</i></p>

<pre xml:space="preserve">
@inproceedings{
  zhou2025mjprefergen,
  title={{MJ}-PreferGen: An Automatic Framework for Preference Data Synthesis},
  author={Yiyang Zhou and Zhaoyang Wang and Tianle Wang and Shangyu Xing and Peng Xia and Bo Li and Kaiyuan Zheng and Zijian Zhang and Zhaorun Chen and Wenhao Zheng and Xuchao Zhang and Chetan Bansal and Weitong Zhang and Ying Wei and Mohit Bansal and Huaxiu Yao},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2025},
  url={https://openreview.net/forum?id=WpZyPk79Fu}
  }
</pre>
    </div>
  </td>
</tr>




<tr>
  <td width="30%" valign="top" align="center">
    <img src="images/paper/csr.png" 
         alt="sym" 
         width="90%"
         style="padding-top:0px; padding-bottom:0px; border-radius:15px;">
  </td>
  <td width="60%" valign="top">
    <p><a href="https://dongjie-cheng.github.io/CSR.html" id="CSR">
    <heading>Calibrated Self-Rewarding Vision Language Models</heading></a><br>
    Yiyang Zhou<sup>*</sup>, Zhiyuan Fan<sup>*</sup>, Dongjie Cheng<sup>*</sup>, Sihan Yang, <b>Zhaorun Chen</b>, Chenhang Cui, Xiyao Wang, Yun Li, Linjun Zhang, and Huaxiu Yao<br>
    Advances in Neural Information Processing Systems (<b>NeurIPS</b>), 2024<br>
    </p>

    <div class="paper" id="csr">
    <a href="https://arxiv.org/pdf/2405.14622">pdf</a> |
    <a href="https://dongjie-cheng.github.io/CSR.html">webpage</a> |
    <a href="javascript:toggleblock('csr_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('csr')" class="togglebib">bibtex</a> |
    <a href="https://arxiv.org/pdf/2405.14622">arXiv</a> |
    <a href="https://www.alphaxiv.org/abs/2405.14622">alphaxiv</a> |
    <a href="https://x.com/HuaxiuYaoML/status/1794203052116680895">blog</a>

    <p align="justify"> <i id="csr_abs">Large Vision-Language Models (LVLMs) have made substantial progress by integrating pre-trained large language models (LLMs) and vision models through instruction tuning. Despite these advancements, LVLMs often exhibit the hallucination phenomenon, where generated text responses appear linguistically plausible but contradict the input image, indicating a misalignment between image and text pairs. This misalignment arises because the model tends to prioritize textual information over visual input, even when both the language model and visual representations are of high quality. Existing methods leverage additional models or human annotations to curate preference data and enhance modality alignment through preference optimization. These approaches are resource-intensive and may not effectively reflect the target LVLM’s preferences, making the curated preferences easily distinguishable. Our work addresses these challenges by proposing the Calibrated Self-Rewarding (CSR) approach, which enables the model to self-improve by iteratively generating candidate responses, evaluating the reward for each response, and curating preference data for fine-tuning. In the reward modeling, we employ a step-wise strategy and incorporate visual constraints into the self-rewarding process to place greater emphasis on visual input. Empirical results demonstrate that CSR significantly enhances performance and reduces hallucinations across ten benchmarks and tasks, achieving substantial improvements over existing methods by 7.62%. Our empirical results are further supported by rigorous theoretical analysis, under mild assumptions, verifying the effectiveness of introducing visual constraints into the self-rewarding paradigm. Additionally, CSR shows compatibility with different vision-language models and the ability to incrementally improve performance through iterative fine-tuning.</i></p>

<pre xml:space="preserve">
@article{zhou2024calibrated,
  title={Calibrated self-rewarding vision language models},
  author={Zhou, Yiyang and Fan, Zhiyuan and Cheng, Dongjie and Yang, Sihan and Chen, Zhaorun and Cui, Chenhang and Wang, Xiyao and Li, Yun and Zhang, Linjun and Yao, Huaxiu},
  journal={arXiv preprint arXiv:2405.14622},
  year={2024}
}
</pre>
    </div>
  </td>
</tr>




<tr></tr>
  <td width="30%" valign="top" align="center">
    <img src="images/escirl/escirl.jpg" 
         alt="sym" 
         width="90%"
         style="padding-top:0px; padding-bottom:0px; border-radius:15px;">
  </td>
  <td width="60%" valign="top">
    <p><a href="https://openreview.net/pdf?id=1IzW0aniyg" id="ESCIRL">
    <heading>EscIRL: Evolving Self-Contrastive IRL for Trajectory Prediction in Autonomous Driving</heading></a><br>
    Siyue Wang<sup>*</sup>, <b>Zhaorun Chen<sup>*</sup></b>, Zhuokai Zhao, Chaoli Mao, Yiyang Zhou, Jiayu He, Albert Hu<br>
    Conference on Robot Learning (<b>CoRL</b>), 2024<br>
    <!-- short version presented at ICLR 2024 <a href="https://iclr-r2fm.github.io/">R2-FM</a> Workshop<br> -->
    </p>

    <div class="paper" id="escirl">
    <a href="https://openreview.net/pdf?id=1IzW0aniyg">pdf</a> |
    <a href="javascript:toggleblock('escirl_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('escirl')" class="togglebib">bibtex</a> |
    <a href="https://openreview.net/pdf?id=1IzW0aniyg">arXiv</a> |
    <!-- <a href="https://x.com/ZRChen_AISafety/status/1802600945567822117">blog</a>  -->

    <p align="justify"> <i id="escirl_abs">While deep neural networks (DNN) and inverse reinforcement learning (IRL) have both been commonly used in autonomous driving to predict trajectories through learning from expert demonstrations, DNN-based methods suffer from data-scarcity, while IRL-based approaches often struggle with generalizability, making both hard to apply to new driving scenarios. To address these issues, we introduce EscIRL, a novel decoupled bi-level training framework that iteratively learns robust reward models from only a few mixed-scenario demonstrations. At the inner level, EscIRL introduces a self-contrastive IRL module that learns a spectrum of specialized reward functions by contrasting demonstrations across different scenarios. At the outer level, ESCIRL employs an evolving loop that iteratively refines the contrastive sets, ensuring global convergence. Experiments on two multi-scenario datasets, CitySim and INTERACTION, demonstrate the effectiveness of EscIRL, outperforming state-of-the-art DNN and IRL-based methods by 41.3% on average. Notably, we show that EscIRL achieves superior generalizability compared to DNN-based approaches while requiring only a small fraction of the data, effectively addressing data-scarcity constraints. All code and data are available at <a href="https://github.com/SiyueWang-CiDi/EscIRL" style="color:#36AE7C;">here</a>.</i></p>

<pre xml:space="preserve">
@inproceedings{wang2024escirl,
  title={EscIRL: Evolving Self-Contrastive IRL for Trajectory Prediction in Autonomous Driving},
  author={Wang, Siyue and Chen, Zhaorun and Zhao, Zhuokai and Mao, Chaoli and Zhou, Yiyang and He, Jiayu and Hu, Albert Sibo},
  booktitle={8th Annual Conference on Robot Learning},
  year={2024}
}
</pre>
    </div>
  </td>
</tr>

<tr>
  <td width="30%" valign="top" align="center">
    <img src="images/autoprm/autoprm.png" 
         alt="sym" 
         width="90%"
         style="padding-top:0px; padding-bottom:0px; border-radius:15px;">
  </td>
  <td width="60%" valign="top">
    <p><a href="https://arxiv.org/abs/2402.11452" id="AUTOPRM">
    <heading>AutoPRM: Automating Procedural Supervision for Multi-Step Reasoning via Controllable Question Decomposition</heading></a><br>
    <b>Zhaorun Chen</b>, Zhuokai Zhao, Zhihong Zhu, Ruiqi Zhang, Xiang Li, Bhiksha Raj and Huaxiu Yao<br>
    North American Chapter of the Association for Computational Linguistics (<b>NAACL</b>), 2024<br>
    <!-- short version presented at ICLR 2024 <a href="https://iclr-r2fm.github.io/">R2-FM</a> Workshop<br> -->
    </p>

    <div class="paper" id="autoprm">
    <a href="https://arxiv.org/pdf/2402.11452.pdf">pdf</a> |
    <a href="javascript:toggleblock('autoprm_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('autoprm')" class="togglebib">bibtex</a> |
    <a href="https://arxiv.org/abs/2402.11452">arXiv</a> |
    <a href="https://www.alphaxiv.org/abs/2402.11452">alphaxiv</a> |
    <a href="https://x.com/ZRChen_AISafety/status/1802600945567822117">blog</a> 

    <p align="justify"> <i id="autoprm_abs">Recent advancements in large language models (LLMs) have shown promise in multi-step reasoning tasks, yet their reliance on extensive manual labeling to provide procedural feedback remains a significant impediment. To address this challenge, in this paper, we propose a novel self-supervised framework AutoPRM that efficiently enhances the fine-tuning of LLMs for intricate reasoning challenges. Specifically, AutoPRM first decomposes complex problems into more manageable subquestions with a controllable granularity switch, then sequentially apply reinforcement learning to iteratively improve the subquestion solver. Additionally, we propose context-guided-decoding to avoid reward tampering and guide the subquestion solver towards the solution of the holistic problem. Extensive experiments show that AutoPRM significantly improves performance on mathematical and commonsense reasoning tasks over SOTA. More encouragingly, AutoPRM can be easily integrated with other orthogonal reasoning pipelines.</i></p>

<pre xml:space="preserve">
@article{chen2024autoprm,
  title={AutoPRM: Automating Procedural Supervision for Multi-Step Reasoning via Controllable Question Decomposition},
  author={Chen, Zhaorun and Zhao, Zhuokai and Zhu, Zhihong and Zhang, Ruiqi and Li, Xiang and Raj, Bhiksha and Yao, Huaxiu},
  journal={arXiv preprint arXiv:2402.11452},
  year={2024}
}
</pre>
    </div>
  </td>
</tr>


<tr>
  <td width="30%" valign="top" align="center">
    <video playsinline 
           autoplay 
           loop 
           muted 
           src="images/acs/ACS-Video.mp4"
           width="90%"
           style="padding-top:0px; padding-bottom:0px; border-radius:15px;">
    </video>
  </td>
  <td width="60%" valign="top">
    <p><a href="https://arxiv.org/abs/2310.03379" id="ACS">
    <heading>Safe Reinforcement Learning via Hierarchical Adaptive Chance-Constraint Safeguards</heading></a><br>
    <b>Zhaorun Chen</b>, Zhuokai Zhao, Tairan He, Binhao Chen, Xuhao Zhao, Liang Gong, Chengliang Liu<br>
    IEEE/RSJ International Conference on Intelligent Robots and Systems (<b>IROS</b>), 2024<br><span style="color:rgb(246, 54, 54)"><b>(Oral Pitch)</b></span>
    </p>

    <div class="paper" id="acs" style="margin-top: -17px;">
    <!-- <a href="https://manipulation-locomotion.github.io">webpage</a> | -->
    <a href="https://arxiv.org/pdf/2310.03379.pdf">pdf</a> |
    <a href="javascript:toggleblock('acs_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('acs')" class="togglebib">bibtex</a> |
    <a href="https://arxiv.org/abs/2310.03379">arXiv</a> 
    <!-- <a href="https://www.alphaxiv.org/abs/2310.03379">alphaxiv</a> | -->

    <p align="justify"> <i id="acs_abs">Ensuring safety in Reinforcement Learning (RL), typically framed as a Constrained Markov Decision Process (CMDP), is crucial for real-world exploration applications. Current approaches in handling CMDP struggle to balance optimality and feasibility, as direct optimization methods cannot ensure state-wise in-training safety, and projection-based methods correct actions inefficiently through lengthy iterations. To address these challenges, we propose Adaptive Chance-constrained Safeguards (ACS), an adaptive, model-free safe RL algorithm using the safety recovery rate as a surrogate chance constraint to iteratively ensure safety during exploration and after achieving convergence. Theoretical analysis indicates that the relaxed probabilistic constraint sufficiently guarantees forward invariance to the safe set. And extensive experiments conducted on both simulated and real-world safety-critical tasks demonstrate its effectiveness in enforcing safety (nearly zero-violation) while preserving optimality (+23.8%), robustness, and fast response in stochastic real-world settings.</i></p>

<pre xml:space="preserve">
@misc{chen2024safereinforcementlearninghierarchical,
  title={Safe Reinforcement Learning via Hierarchical Adaptive Chance-Constraint Safeguards}, 
  author={Zhaorun Chen and Zhuokai Zhao and Tairan He and Binhao Chen and Xuhao Zhao and Liang Gong and Chengliang Liu},
  year={2024},
  eprint={2310.03379},
  archivePrefix={arXiv},
  primaryClass={cs.RO},
  url={https://arxiv.org/abs/2310.03379}, 
}
</pre>
    </div>
  </td>
</tr>



<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Preprints </sectionheading>(see full list on <a href="https://scholar.google.com/citations?user=UZg5N5UAAAAJ">Google Scholar</a>)</td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">
<br/>

<tr>
  <td width="30%" valign="top" align="center">
    <img src="images/mjvideo/overview.png" 
         alt="sym" 
         width="90%"
         style="padding-top:0px; padding-bottom:0px; border-radius:15px;">
  </td>
  <td width="60%" valign="top">
    <p><a href="https://aiming-lab.github.io/MJ-VIDEO.github.io/" id="mjvideo">
    <heading>MJ-Video: Benchmarking and Rewarding Video Generation with Fine-Grained Video Preference</heading></a><br>
    Haibo Tong<sup>*</sup>, Zhaoyang Wang<sup>*</sup>, <b>Zhaorun Chen</b>, Haonian Ji, Shi Qiu, Siwei Han, Kexin Geng, Zhongkai Xue, Yiyang Zhou, Peng Xia, Mingyu Ding, Rafael Rafailov, Chelsea Finn, and Huaxiu Yao<br>
    In submission, 2025<br>
    </p>

    <div class="paper" id="mjvideo">
    <a href="https://arxiv.org/pdf/2502.01719">pdf</a> |
    <a href="https://aiming-lab.github.io/MJ-VIDEO.github.io/">webpage</a> |
    <a href="javascript:toggleblock('mjvideo_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('mjvideo')" class="togglebib">bibtex</a> |
    <a href="https://arxiv.org/abs/2502.01719">arXiv</a> |
    <a href="https://www.alphaxiv.org/abs/2502.01719">alphaxiv</a> |
    <a href="">blog</a> 

    <p align="justify"> <i id="mjvideo_abs">Recent advancements in video generation have significantly improved the ability to synthesize videos from text instructions. However, existing models still struggle with key challenges such as instruction misalignment, content hallucination, safety concerns, and bias. Addressing these limitations, we introduce MJ-BENCH-VIDEO, a large-scale video preference benchmark designed to evaluate video generation across five critical aspects: Alignment, Safety, Fineness, Coherence & Consistency, and Bias & Fairness. This benchmark incorporates 28 fine-grained criteria to provide a comprehensive evaluation of video preference. Building upon this dataset, we propose MJ-VIDEO, a Mixture-of-Experts (MoE)-based video reward model designed to deliver fine-grained reward. MJ-VIDEO can dynamically select relevant experts to accurately judge the preference based on the input text-video pair. This architecture enables more precise and adaptable preference judgments. Through extensive benchmarking on MJ-BENCH-VIDEO, we analyze the limitations of existing video reward models and demonstrate the superior performance of MJ-VIDEO in video preference assessment, achieving 17.58% and 15.87% improvements in overall and fine-grained preference judgments, respectively. Additionally, introducing MJ-VIDEO for preference tuning in video generation enhances the alignment performance.<i></p>

<pre xml:space="preserve">
  @misc{tong2025mjvideofinegrainedbenchmarkingrewarding,
    title={MJ-VIDEO: Fine-Grained Benchmarking and Rewarding Video Preferences in Video Generation}, 
    author={Haibo Tong and Zhaoyang Wang and Zhaorun Chen and Haonian Ji and Shi Qiu and Siwei Han and Kexin Geng and Zhongkai Xue and Yiyang Zhou and Peng Xia and Mingyu Ding and Rafael Rafailov and Chelsea Finn and Huaxiu Yao},
    year={2025},
    eprint={2502.01719},
    archivePrefix={arXiv},
    primaryClass={cs.CV},
    url={https://arxiv.org/abs/2502.01719}, 
}
</pre>
    </div>
  </td>
</tr>

<tr>
  <td width="30%" valign="top" align="center">
    <video playsinline 
           autoplay 
           loop 
           muted 
           src="images/grape/grape.mp4"
           width="90%"
           style="padding-top:0px; padding-bottom:0px; border-radius:15px;">
    </video>
  </td>
  <td width="60%" valign="top">
    <p><a href="https://grape-vla.github.io/" id="SafeWatch">
    <heading>GRAPE: Generalizing Robot Policy via Preference Alignment</heading></a><br>
    Zijian Zhang<sup>*</sup>, Kaiyuan Zheng<sup>*</sup>, <b>Zhaorun Chen<sup>*</sup></b>, Joel Jang, Yi Li, Chaoqi Wang, Mingyu Ding, Dieter Fox, and Huaxiu Yao<br>
    In submission, 2025<br>
    </p>

    <div class="paper" id="grape">
    <a href="https://arxiv.org/pdf/2411.19309">pdf</a> |
    <a href="https://grape-vla.github.io/">webpage</a> |
    <a href="javascript:toggleblock('grape_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('grape')" class="togglebib">bibtex</a> |
    <a href="https://arxiv.org/pdf/2411.19309">arXiv</a> |
    <a href="https://www.alphaxiv.org/abs/2411.19309">alphaxiv</a> |
    <a href="https://x.com/HuaxiuYaoML/status/1863708749732680162">blog</a> 

    <p align="justify"> <i id="grape_abs">Despite the recent advancements of vision-language-action (VLA) models on a variety of robotics tasks, they suffer from critical issues such as poor generalizability to unseen tasks, due to their reliance on behavior cloning exclusively from successful rollouts. Furthermore, they are typically fine-tuned to replicate demonstrations collected by experts under different settings, thus introducing distribution bias and limiting their adaptability to diverse manipulation objectives, such as efficiency, safety, and task completion. To bridge this gap, we introduce GRAPE: Generalizing Robot Policy via Preference Alignment. Specifically, GRAPE aligns VLAs on a trajectory level and implicitly models reward from both successful and failure trials to boost generalizability to diverse tasks. Moreover, GRAPE breaks down complex manipulation tasks to independent stages and automatically guides preference modeling through customized spatiotemporal constraints with keypoints proposed by a large vision-language model. Notably, these constraints are flexible and can be customized to align the model with varying objectives, such as safety, efficiency, or task success. We evaluate GRAPE across a diverse array of tasks in both real-world and simulated environments. Experimental results demonstrate that GRAPE enhances the performance of state-of-the-art VLA models, increasing success rates on in-domain and unseen manipulation tasks by 51.79% and 60.36%, respectively. Additionally, GRAPE can be aligned with various objectives, such as safety and efficiency, reducing collision rates by 44.31% and rollout step-length by 11.15%, respectively. All code, models, and data are available at <a href="https://grape-vla.github.io/" style="color:#36AE7C;">here</a>.</i></p>

<pre xml:space="preserve">
@article{zhang2024grape,
  title={Grape: Generalizing robot policy via preference alignment},
  author={Zhang, Zijian and Zheng, Kaiyuan and Chen, Zhaorun and Jang, Joel and Li, Yi and Wang, Chaoqi and Ding, Mingyu and Fox, Dieter and Yao, Huaxiu},
  journal={arXiv preprint arXiv:2411.19309},
  year={2024}
}
</pre>
    </div>
  </td>
</tr>

<tr>
  <td width="30%" valign="top" align="center">
    <img src="images/mjbench/overview.png" 
         alt="sym" 
         width="90%"
         style="padding-top:0px; padding-bottom:0px; border-radius:15px;">
  </td>
  <td width="60%" valign="top">
    <p><a href="https://mj-bench.github.io/" id="mjbench">
    <heading>MJ-Bench: Is Your Multimodal Reward Model Really a Good Judge for Text-to-Image Generation?</heading></a><br>
    <b>Zhaorun Chen<sup>*</sup></b>, Yichao Du<sup>*</sup>, Zichen Wen<sup>*</sup>, Yiyang Zhou<sup>*</sup>, Chenhang Cui, Zhenzhen Weng, Haoqin Tu, Chaoqi Wang, Zhengwei Tong, Qinglan Huang, Canyu Chen, Qinghao Ye, Zhihong Zhu, Yuqing Zhang, Jiawei Zhou, Zhuokai Zhao, Rafael Rafailov, Chelsea Finn, and Huaxiu Yao<br>
    In submission, 2025<br>
    </p>

    <div class="paper" id="mjbench">
    <a href="https://arxiv.org/pdf/2407.04842">pdf</a> |
    <a href="https://mj-bench.github.io/">webpage</a> |
    <a href="javascript:toggleblock('mjbench_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('mjbench')" class="togglebib">bibtex</a> |
    <a href="https://arxiv.org/abs/2407.04842">arXiv</a> |
    <a href="https://www.alphaxiv.org/abs/2407.04842">alphaxiv</a> |
    <a href="https://x.com/HuaxiuYaoML/status/1810728309182861462">blog</a> 

    <p align="justify"> <i id="mjbench_abs">Multimodal reward models (RMs) are critical in RLHF and RLAIF, where they serve as judges and provide feedback for aligning foundation models (FMs) with desired behaviors. Despite their significance, these multimodal judges often un- dergo inadequate evaluation of their capabilities and biases, which may lead to potential misalignment and unsafe fine-tuning outcomes. To address this issue, we introduce MJ-Bench, a novel benchmark which incorporates a comprehensive preference dataset to evaluate multimodal judges in providing feedback for image generation models across four key perspectives: alignment, safety, image quality, and bias. Specifically, we evaluate a large variety of multimodal judges includ- ing smaller-sized CLIP-based scoring models, open-source VLMs (e.g. LLaVA family), and close-source VLMs (e.g. GPT-4o, Claude 3) on each decomposed subcategory of our preference dataset. Experiments reveal that close-source VLMs generally provide better feedback, with GPT-4o outperforming other judges in aver- age. Compared with open-source VLMs, smaller-sized scoring models can provide better feedback regarding text-image alignment and image quality, while VLMs provide more accurate feedback regarding safety and generation bias due to their stronger reasoning capabilities. Notably, human evaluations on end-to-end fine- tuned models using separate feedback from these multimodal judges provide similar conclusions, further confirming the effectiveness of MJ-Bench. Further studies in feedback scale reveal that VLM judges can generally provide more accurate and stable feedback in natural language (Likert-scale) than numerical scales. The code and data are available <a href="https://github.com/BillChan226/MJ-Bench" style="color:#36AE7C;">here</a>.</i></p>

<pre xml:space="preserve">
@misc{chen2024mjbenchmultimodalrewardmodel,
  title={MJ-Bench: Is Your Multimodal Reward Model Really a Good Judge for Text-to-Image Generation?}, 
  author={Zhaorun Chen and Yichao Du and Zichen Wen and Yiyang Zhou and Chenhang Cui and Zhenzhen Weng and Haoqin Tu and Chaoqi Wang and Zhengwei Tong and Qinglan Huang and Canyu Chen and Qinghao Ye and Zhihong Zhu and Yuqing Zhang and Jiawei Zhou and Zhuokai Zhao and Rafael Rafailov and Chelsea Finn and Huaxiu Yao},
  year={2024},
  eprint={2407.04842},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2407.04842}, 
}
</pre>
    </div>
  </td>
</tr>

</table>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Organizer</sectionheading></td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">
  <tr>
    <td style="padding:20px;width:100%;vertical-align:middle">
      <p>
      <a href="https://www.llmagentsafetycomp24.com/">The LLM and Agent Safety Competition (CLAS 2024)</a>, NeurIPS 2024
      </p>
    </td>
  </tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Reviewer Service</sectionheading></td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">
  <tr>
    <td style="padding:20px;width:100%;vertical-align:middle">
      Conference Reviewer: NeurIPS'25, ICML'25, ICCV'25, CVPR'25, NeurIPS'24, ICLR'24'25, COLM'24, ARR'24, IROS'24
      <!-- <br>
      Journal Reviewer: Plant Phenomics -->
      </p>
    </td>
  </tr>
</table>



<!-- 
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tbody>
      <tr>
          <td style="padding:0px">
              <br>
              <br>
              <div>
                  <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=080808&w=200&t=tt&d=1LQ8joet7HZOF5z74B4awh9KBpyFKrjXiK4F3y83o_g&co=ffffff&cmo=3acc3a&cmn=ff5353&ct=808080"></script>
             </div>
          </td>
      </tr>
  </tbody>
</table>
 -->


<hr/>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="2">
    <tr><td><br><p align="right"><font size="1.5">
    Modified version of template from <a href="http://www.cs.berkeley.edu/~barron/">here</a>
    </font></p></td></tr>
</table>

  </td></tr>
</table>


<script xml:space="preserve" language="JavaScript">
  hideallbibs();
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('material_review_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('ieee_iot_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('mjvideo_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('fisao_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('mmie_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('mmdt_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('anyprefer_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('safewatch_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('grape_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('agentpoison_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('csr_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('mjbench_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('escirl_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('halc_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('autoprm_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
    hideblock('acs_abs');
  </script>
  <script>
  document.addEventListener("DOMContentLoaded", function() {
    const lazyMedia = document.querySelectorAll(".lazy-media");
    
    const lazyLoadMedia = (entries, observer) => {
      entries.forEach(entry => {
        if (entry.isIntersecting) {
          const media = entry.target;
          media.src = media.dataset.src;
          media.classList.add("loaded");
          observer.unobserve(media);
        }
      });
    };
  
    const mediaObserver = new IntersectionObserver(lazyLoadMedia, {
      root: null,
      threshold: 0.1
    });
  
    lazyMedia.forEach(media => mediaObserver.observe(media));
  });
</script>
</body>

</html>